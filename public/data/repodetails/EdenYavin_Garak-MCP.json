{
  "mcp_name": "EdenYavin/Garak-MCP",
  "mcp_description": "Facilitates vulnerability scanning on various LLMs using Garak through a lightweight MCP server.",
  "mcp_id": "EdenYavin_Garak-MCP",
  "fetch_timestamp": "2025-06-23T02:47:00.330506Z",
  "github_url": "https://github.com/EdenYavin/Garak-MCP",
  "repository": {
    "name": "Garak-MCP",
    "full_name": "EdenYavin/Garak-MCP",
    "description": "MCP Server for using Garak LLM vulnerability scanner",
    "html_url": "https://github.com/EdenYavin/Garak-MCP",
    "created_at": "2025-04-13T15:22:08Z",
    "updated_at": "2025-05-13T17:19:14Z",
    "pushed_at": "2025-04-19T09:35:25Z",
    "size": 293,
    "stargazers_count": 1,
    "watchers_count": 1,
    "forks_count": 1,
    "open_issues_count": 0,
    "language": "Python",
    "license": "MIT License",
    "topics": [],
    "default_branch": "main",
    "owner": {
      "login": "EdenYavin",
      "type": "User",
      "avatar_url": "https://avatars.githubusercontent.com/u/64005996?v=4",
      "html_url": "https://github.com/EdenYavin"
    },
    "has_issues": true,
    "has_projects": true,
    "has_downloads": true,
    "has_wiki": true,
    "has_pages": false,
    "archived": false,
    "disabled": false,
    "visibility": "public",
    "network_count": 1,
    "subscribers_count": 1,
    "languages": {
      "Python": 12593
    },
    "language_percentages": {
      "Python": 100
    },
    "pull_requests_count": 0,
    "contributors_count": 1
  },
  "readme": "# MCP Server For Garak LLM Vulnerability Scanner \n\nA lightweight MCP (Model Context Protocol) server for Garak.\n\nExample:\n\nhttps://github.com/user-attachments/assets/f6095d26-2b79-4ef7-a889-fd6be27bbbda\n\n\n---\n\n## Tools Provided\n\n### Overview\n| Name | Description |\n|------|-------------|\n| list_model_types | List all available model types (ollama, openai, huggingface, ggml) |\n| list_models | List all available models for a given model type |\n| list_garak_probes | List all available Garak attacks/probes |\n| get_report | Get the report of the last run |\n| run_attack | Run an attack with a given model and probe |\n\n### Detailed Description\n\n- **list_model_types**\n  - List all available model types that can be used for attacks\n  - Returns a list of supported model types (ollama, openai, huggingface, ggml)\n\n- **list_models**\n  - List all available models for a given model type\n  - Input parameters:\n    - `model_type` (string, required): The type of model to list (ollama, openai, huggingface, ggml)\n  - Returns a list of available models for the specified type\n\n- **list_garak_probes**\n  - List all available Garak attacks/probes\n  - Returns a list of available probes/attacks that can be run\n\n- **get_report**\n  - Get the report of the last run\n  - Returns the path to the report file\n\n- **run_attack**\n  - Run an attack with the given model and probe\n  - Input parameters:\n    - `model_type` (string, required): The type of model to use\n    - `model_name` (string, required): The name of the model to use\n    - `probe_name` (string, required): The name of the attack/probe to use\n  - Returns a list of vulnerabilities found\n\n---\n\n## Prerequisites\n\n1. **Python 3.11 or higher**: This project requires Python 3.11 or newer.\n   ```bash\n   # Check your Python version\n   python --version\n   ```\n\n2. **Install uv**: A fast Python package installer and resolver.\n   ```bash\n   pip install uv\n   ```\n   Or use Homebrew:\n   ```bash\n   brew install uv\n   ```\n3. **Optional: Ollama**: If you want to run attacks on ollama models be sure that the ollama server is running.\n\n```bash\nollama serve\n```\n\n---\n\n## Installation\n\n1. Clone this repository:\n```bash\ngit clone https://github.com/BIGdeadLock/Garak-MCP.git\n```\n2. Configure your MCP Host (Claude Desktop ,Cursor, etc): \n\n```json\n{\n  \"mcpServers\": {\n    \"garak-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"path-to/Garak-MCP\", \"run\", \"garak-server\"],\n      \"env\": {}\n    }\n  }\n}\n\n```\n---\nTested on:\n- [X] Cursor\n- [X] Claude Desktop\n\n---\n## Future Steps\n\n- [ ] Add support for Smithery AI: Docker and config\n- [ ] Improve Reporting\n- [ ] Test and validate OpenAI models (GPT-3.5, GPT-4)\n- [ ] Test and validate HuggingFace models\n- [ ] Test and validate local GGML models\n"
}