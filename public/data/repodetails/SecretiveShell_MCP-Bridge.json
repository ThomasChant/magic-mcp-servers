{
  "mcp_name": "MCP-Bridge",
  "mcp_description": "",
  "mcp_id": "SecretiveShell_MCP-Bridge",
  "fetch_timestamp": "2025-06-23T06:32:27.144720Z",
  "github_url": "https://github.com/SecretiveShell/MCP-Bridge",
  "repository": {
    "name": "MCP-Bridge",
    "full_name": "SecretiveShell/MCP-Bridge",
    "description": "A middleware to provide an openAI compatible endpoint that can call MCP tools",
    "html_url": "https://github.com/SecretiveShell/MCP-Bridge",
    "created_at": "2024-11-30T19:29:35Z",
    "updated_at": "2025-06-23T02:14:57Z",
    "pushed_at": "2025-06-16T16:54:34Z",
    "size": 529,
    "stargazers_count": 778,
    "watchers_count": 778,
    "forks_count": 95,
    "open_issues_count": 25,
    "language": "Python",
    "license": "MIT License",
    "topics": [
      "ai",
      "claude",
      "mcp",
      "mcp-server",
      "mcp-servers",
      "model-context-protocol",
      "openai",
      "openai-api"
    ],
    "default_branch": "master",
    "owner": {
      "login": "SecretiveShell",
      "type": "User",
      "avatar_url": "https://avatars.githubusercontent.com/u/84923604?v=4",
      "html_url": "https://github.com/SecretiveShell"
    },
    "has_issues": true,
    "has_projects": true,
    "has_downloads": true,
    "has_wiki": true,
    "has_pages": false,
    "archived": false,
    "disabled": false,
    "visibility": "public",
    "network_count": 95,
    "subscribers_count": 11,
    "languages": {
      "Python": 81602,
      "Smarty": 1812,
      "Dockerfile": 591,
      "HCL": 415
    },
    "language_percentages": {
      "Python": 96.66,
      "Smarty": 2.15,
      "Dockerfile": 0.7,
      "HCL": 0.49
    },
    "pull_requests_count": 49,
    "contributors_count": 13,
    "latest_release": {
      "tag_name": "0.5.1",
      "name": "0.5.1",
      "published_at": "2025-02-08T22:32:33Z",
      "body": "## What's Changed\n* Bump actions/download-artifact from 4.1.7 to 4.1.8 in /.github/workflows by @dependabot in https://github.com/SecretiveShell/MCP-Bridge/pull/43\n* Bump docker/login-action from 2 to 3 in /.github/workflows by @dependabot in https://github.com/SecretiveShell/MCP-Bridge/pull/44\n* do not force str on tool endpoints by @SecretiveShell in https://github.com/SecretiveShell/MCP-Bridge/pull/46\n\n## New Contributors\n* @dependabot made their first contribution in https://github.com/SecretiveShell/MCP-Bridge/pull/43\n\n**Full Changelog**: https://github.com/SecretiveShell/MCP-Bridge/compare/0.5.0...0.5.1",
      "prerelease": false,
      "draft": false
    },
    "tags": [
      {
        "name": "0.5.1",
        "commit_sha": "59ac1368dd755fe438f7c56ebaa0784ec872c42d"
      },
      {
        "name": "0.5.0",
        "commit_sha": "3c35f8d4569dbfdbf07d69c54302aec698c6bc46"
      },
      {
        "name": "0.4.0",
        "commit_sha": "d84278e40ead595a8a34a58884820107601d4edd"
      },
      {
        "name": "0.3.0",
        "commit_sha": "7d85cfe29d672468322d33b06cb1cbe2a95656fd"
      },
      {
        "name": "0.2.2",
        "commit_sha": "23c7f37b37b6c6433d14bee3df98e7bd1423a0c7"
      },
      {
        "name": "0.2.1",
        "commit_sha": "84f0ae1e5cf5cbf26f33bf3e199cfdfacc37858c"
      },
      {
        "name": "0.2.0",
        "commit_sha": "d50e9fee372a246ca14922ee7db950c7b9336cc1"
      },
      {
        "name": "0.1.0",
        "commit_sha": "3502453a0fe8fdccf9628c251b42a8f154788652"
      }
    ],
    "latest_version": "0.5.1"
  },
  "readme": "# MCP-Bridge\n\n<p>\n  <a href=\"https://discord.gg/4NVQHqNxSZ\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/1320517159331430480?style=flat&logo=discord&color=blue\"></a>\n  <a href=\"/docs/README.md\"><img alt=\"Static Badge\" src=\"https://img.shields.io/badge/docs-md-blue\"></a>\n  <a href=\"LICENSE\"><img alt=\"Static Badge\" src=\"https://img.shields.io/badge/License-MIT-blue?style=flat\"></a>\n</p>\n\n\nMCP-Bridge acts as a bridge between the OpenAI API and MCP (MCP) tools, allowing developers to leverage MCP tools through the OpenAI API interface.\n\n## Overview\nMCP-Bridge is designed to facilitate the integration of MCP tools with the OpenAI API. It provides a set of endpoints that can be used to interact with MCP tools in a way that is compatible with the OpenAI API. This allows you to use any client with any MCP tool without explicit support for MCP. For example, see this example of using Open Web UI with the official MCP fetch tool. \n\n![open web ui example](/assets/owui_example.png)\n\n## Current Features\n\nworking features:\n\n- non streaming chat completions with MCP\n- streaming chat completions with MCP\n\n- non streaming completions without MCP\n\n- MCP tools\n- MCP sampling\n\n- SSE Bridge for external clients\n\nplanned features:\n\n- streaming completions are not implemented yet\n\n- MCP resources are planned to be supported\n\n## Installation\n\nThe recommended way to install MCP-Bridge is to use Docker. See the example compose.yml file for an example of how to set up docker. \n\nNote that this requires an inference engine with tool call support. I have tested this with vLLM with success, though ollama should also be compatible.\n\n### Docker installation\n\n1. **Clone the repository**\n\n2. **Edit the compose.yml file**\n\nYou will need to add a reference to the config.json file in the compose.yml file. Pick any of\n- add the config.json file to the same directory as the compose.yml file and use a volume mount (you will need to add the volume manually)\n- add a http url to the environment variables to download the config.json file from a url\n- add the config json directly as an environment variable\n\nsee below for an example of each option:\n```bash\nenvironment:\n  - MCP_BRIDGE__CONFIG__FILE=config.json # mount the config file for this to work\n  - MCP_BRIDGE__CONFIG__HTTP_URL=http://10.88.100.170:8888/config.json\n  - MCP_BRIDGE__CONFIG__JSON={\"inference_server\":{\"base_url\":\"http://example.com/v1\",\"api_key\":\"None\"},\"mcp_servers\":{\"fetch\":{\"command\":\"uvx\",\"args\":[\"mcp-server-fetch\"]}}}\n```\nThe mount point for using the config file would look like:\n```yaml\n    volumes:\n      - ./config.json:/mcp_bridge/config.json\n```\n\n3. **run the service**\n```\ndocker-compose up --build -d\n```\n\n### Manual installation (no docker)\n\nIf you want to run the application without docker, you will need to install the requirements and run the application manually.\n\n1. **Clone the repository**\n\n2. **Set up a dependencies:**\n```bash\nuv sync\n```\n\n3. **Create a config.json file in the root directory**\n\nHere is an example config.json file:\n```json\n{\n   \"inference_server\": {\n      \"base_url\": \"http://example.com/v1\",\n      \"api_key\": \"None\"\n   },\n   \"mcp_servers\": {\n      \"fetch\": {\n        \"command\": \"uvx\",\n        \"args\": [\"mcp-server-fetch\"]\n      }\n   }\n}\n```\n\n4. **Run the application:**\n```bash\nuv run mcp_bridge/main.py\n```\n\n## Usage\nOnce the application is running, you can interact with it using the OpenAI API.\n\nView the documentation at [http://yourserver:8000/docs](http://localhost:8000/docs). There is an endpoint to list all the MCP tools available on the server, which you can use to test the application configuration.\n\n## Rest API endpoints\n\nMCP-Bridge exposes many rest api endpoints for interacting with all of the native MCP primatives. This lets you outsource the complexity of dealing with MCP servers to MCP-Bridge without comprimising on functionality. See the openapi docs for examples of how to use this functionality.\n\n## SSE Bridge\nMCP-Bridge also provides an SSE bridge for external clients. This lets external chat apps with explicit MCP support use MCP-Bridge as a MCP server. Point your client at the SSE endpoint (http://yourserver:8000/mcp-server/sse) and you should be able to see all the MCP tools available on the server.\n\nThis also makes it easy to test if your configuration is working correctly. You can use [wong2/mcp-cli](https://github.com/wong2/mcp-cli?tab=readme-ov-file#connect-to-a-running-server-over-sse) to test your configuration. `npx @wong2/mcp-cli --sse http://localhost:8000/mcp-server/sse`\n\nIf you want to use the tools inside of [claude desktop](https://claude.ai/download) or other `STDIO` only MCP clients, you can do this with a tool such as [lightconetech/mcp-gateway](https://github.com/lightconetech/mcp-gateway)\n\n## Configuration\n\nTo add new MCP servers, edit the config.json file.\n\n### API Key Authentication\n\nMCP-Bridge supports API key authentication to secure your server. To enable this feature, add something like this to your `config.json` file:\n\n```json\n{\n    \"security\": {\n      \"auth\": {\n        \"enabled\": true,\n        \"api_keys\": [\n          {\n            \"key\": \"your-secure-api-key-here\"\n          }\n        ]\n      }\n    }\n}\n```\n\nWhen making requests to the MCP-Bridge server, include the API key in the Authorization header as a Bearer token:\n\n```\nAuthorization: Bearer your-secure-api-key-here\n```\n\nIf the `api_key` field is empty or not present in the configuration, authentication will be skipped, allowing backward compatibility.\n\n### Full Configuration Example\n\nan example config.json file with most of the options explicitly set:\n\n```json\n{\n    \"inference_server\": {\n        \"base_url\": \"http://localhost:8000/v1\",\n        \"api_key\": \"None\"\n    },\n    \"sampling\": {\n        \"timeout\": 10,\n        \"models\": [\n            {\n                \"model\": \"gpt-4o\",\n                \"intelligence\": 0.8,\n                \"cost\": 0.9,\n                \"speed\": 0.3\n            },\n            {\n                \"model\": \"gpt-4o-mini\",\n                \"intelligence\": 0.4,\n                \"cost\": 0.1,\n                \"speed\": 0.7\n            }\n        ]\n    },\n    \"mcp_servers\": {\n        \"fetch\": {\n            \"command\": \"uvx\",\n            \"args\": [\n                \"mcp-server-fetch\"\n            ]\n        }\n    },\n    \"security\": {\n      \"auth\": {\n        \"enabled\": true,\n        \"api_keys\": [\n          {\n            \"key\": \"your-secure-api-key-here\"\n          }\n        ]\n      }\n    },\n    \"network\": {\n        \"host\": \"0.0.0.0\",\n        \"port\": 9090\n    },\n    \"logging\": {\n        \"log_level\": \"DEBUG\"\n    }\n}\n```\n\n| Section          | Description                        |\n| ---------------- | ---------------------------------- |\n| inference_server | The inference server configuration |\n| mcp_servers      | The MCP servers configuration      |\n| network          | uvicorn network configuration      |\n| logging          | The logging configuration          |\n| api_key          | API key for server authentication  |\n\n## Support\n\nIf you encounter any issues please open an issue or join the [discord](https://discord.gg/4NVQHqNxSZ).\n\nThere is also documentation available [here](/docs/README.md).\n\n## How does it work\n\nThe application sits between the OpenAI API and the inference engine. An incoming request is modified to include tool definitions for all MCP tools available on the MCP servers. The request is then forwarded to the inference engine, which uses the tool definitions to create tool calls. MCP bridge then manage the calls to the tools. The request is then modified to include the tool call results, and is returned to the inference engine again so the LLM can create a response. Finally, the response is returned to the OpenAI API.\n\n```mermaid\nsequenceDiagram\n    participant OpenWebUI as Open Web UI\n    participant MCPProxy as MCP Proxy\n    participant MCPserver as MCP Server\n    participant InferenceEngine as Inference Engine\n\n    OpenWebUI ->> MCPProxy: Request\n    MCPProxy ->> MCPserver: list tools\n    MCPserver ->> MCPProxy: list of tools\n    MCPProxy ->> InferenceEngine: Forward Request\n    InferenceEngine ->> MCPProxy: Response\n    MCPProxy ->> MCPserver: call tool\n    MCPserver ->> MCPProxy: tool response\n    MCPProxy ->> InferenceEngine: llm uses tool response\n    InferenceEngine ->> MCPProxy: Response\n    MCPProxy ->> OpenWebUI: Return Response\n```\n\n## Contribution Guidelines\nContributions to MCP-Bridge are welcome! To contribute, please follow these steps:\n1. Fork the repository.\n2. Create a new branch for your feature or bug fix.\n3. Make your changes and commit them.\n4. Push your changes to your fork.\n5. Create a pull request to the main repository.\n\n## License\nMCP-Bridge is licensed under the MIT License. See the [LICENSE](LICENSE) file for more information.\n"
}