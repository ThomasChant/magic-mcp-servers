{
  "mcp_name": "WebSearch-MCP",
  "mcp_description": "Self-hosted Websearch service.",
  "mcp_id": "mnhlt_WebSearch-MCP",
  "fetch_timestamp": "2025-06-23T09:24:03.438077Z",
  "github_url": "https://github.com/mnhlt/WebSearch-MCP",
  "repository": {
    "name": "WebSearch-MCP",
    "full_name": "mnhlt/WebSearch-MCP",
    "description": "[Self-hosted] A Model Context Protocol (MCP) server implementation that provides a web search capability over stdio transport. This server integrates with a WebSearch Crawler API to retrieve search results.",
    "html_url": "https://github.com/mnhlt/WebSearch-MCP",
    "created_at": "2025-03-24T21:44:03Z",
    "updated_at": "2025-06-23T05:37:39Z",
    "pushed_at": "2025-04-30T18:04:26Z",
    "size": 69,
    "stargazers_count": 12,
    "watchers_count": 12,
    "forks_count": 1,
    "open_issues_count": 0,
    "language": "JavaScript",
    "license": null,
    "topics": [
      "internet-access-llm",
      "websearch"
    ],
    "default_branch": "main",
    "owner": {
      "login": "mnhlt",
      "type": "User",
      "avatar_url": "https://avatars.githubusercontent.com/u/11975819?v=4",
      "html_url": "https://github.com/mnhlt"
    },
    "has_issues": true,
    "has_projects": true,
    "has_downloads": true,
    "has_wiki": true,
    "has_pages": false,
    "archived": false,
    "disabled": false,
    "visibility": "public",
    "network_count": 1,
    "subscribers_count": 1,
    "languages": {
      "JavaScript": 8802,
      "Dockerfile": 484
    },
    "language_percentages": {
      "JavaScript": 94.79,
      "Dockerfile": 5.21
    },
    "pull_requests_count": 1,
    "contributors_count": 3,
    "latest_release": {
      "tag_name": "v1.0.3",
      "name": "v1.0.3",
      "published_at": "2025-04-30T18:05:06Z",
      "body": "* Add npm publish action\r\n**Full Changelog**: https://github.com/mnhlt/WebSearch-MCP/compare/v1.0.2...v1.0.3",
      "prerelease": false,
      "draft": false
    },
    "tags": [
      {
        "name": "v1.0.3",
        "commit_sha": "79a9c5bdf404556d325731e929f7a78757c24524"
      },
      {
        "name": "v1.0.2",
        "commit_sha": "ac565d1588495346867ca154bfcb4ac1f4dd469c"
      }
    ],
    "latest_version": "v1.0.3",
    "package_json_version": "1.0.2"
  },
  "readme": "# WebSearch-MCP\n\n[![smithery badge](https://smithery.ai/badge/@mnhlt/WebSearch-MCP)](https://smithery.ai/server/@mnhlt/WebSearch-MCP)\n\nA Model Context Protocol (MCP) server implementation that provides a web search capability over stdio transport. This server integrates with a WebSearch Crawler API to retrieve search results.\n\n## Table of Contents\n\n- [About](#about)\n- [Installation](#installation)\n- [Configuration](#configuration)\n- [Setup & Integration](#setup--integration)\n  - [Setting Up the Crawler Service](#setting-up-the-crawler-service)\n    - [Prerequisites](#prerequisites)\n    - [Starting the Crawler Service](#starting-the-crawler-service)\n    - [Testing the Crawler API](#testing-the-crawler-api)\n    - [Custom Configuration](#custom-configuration)\n  - [Integrating with MCP Clients](#integrating-with-mcp-clients)\n    - [Quick Reference: MCP Configuration](#quick-reference-mcp-configuration)\n    - [Claude Desktop](#claude-desktop)\n    - [Cursor IDE](#cursor-ide)\n    - [Cline](#cline-command-line-interface-for-claude)\n- [Usage](#usage)\n  - [Parameters](#parameters)\n  - [Example Search Response](#example-search-response)\n  - [Testing Locally](#testing-locally)\n  - [As a Library](#as-a-library)\n- [Troubleshooting](#troubleshooting)\n  - [Crawler Service Issues](#crawler-service-issues)\n  - [MCP Server Issues](#mcp-server-issues)\n- [Development](#development)\n  - [Project Structure](#project-structure)\n  - [Publishing to npm](#publishing-to-npm)\n- [Contributing](#contributing)\n- [License](#license)\n\n## About\n\nWebSearch-MCP is a Model Context Protocol server that provides web search capabilities to AI assistants that support MCP. It allows AI models like Claude to search the web in real-time, retrieving up-to-date information about any topic.\n\nThe server integrates with a Crawler API service that handles the actual web searches, and communicates with AI assistants using the standardized Model Context Protocol.\n\n## Installation\n\n### Installing via Smithery\n\nTo install WebSearch for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mnhlt/WebSearch-MCP):\n\n```bash\nnpx -y @smithery/cli install @mnhlt/WebSearch-MCP --client claude\n```\n\n### Manual Installation\n```bash\nnpm install -g websearch-mcp\n```\n\nOr use without installing:\n\n```bash\nnpx websearch-mcp\n```\n\n## Configuration\n\nThe WebSearch MCP server can be configured using environment variables:\n\n- `API_URL`: The URL of the WebSearch Crawler API (default: `http://localhost:3001`)\n- `MAX_SEARCH_RESULT`: Maximum number of search results to return when not specified in the request (default: `5`)\n\nExamples:\n```bash\n# Configure API URL\nAPI_URL=https://crawler.example.com npx websearch-mcp\n\n# Configure maximum search results\nMAX_SEARCH_RESULT=10 npx websearch-mcp\n\n# Configure both\nAPI_URL=https://crawler.example.com MAX_SEARCH_RESULT=10 npx websearch-mcp\n```\n\n## Setup & Integration\n\nSetting up WebSearch-MCP involves two main parts: configuring the crawler service that performs the actual web searches, and integrating the MCP server with your AI client applications.\n\n### Setting Up the Crawler Service\n\nThe WebSearch MCP server requires a crawler service to perform the actual web searches. You can easily set up the crawler service using Docker Compose.\n\n### Prerequisites\n\n- [Docker](https://www.docker.com/get-started) and [Docker Compose](https://docs.docker.com/compose/install/)\n\n### Starting the Crawler Service\n\n1. Create a file named `docker-compose.yml` with the following content:\n\n```yaml\nversion: '3.8'\n\nservices:\n  crawler:\n    image: laituanmanh/websearch-crawler:latest\n    container_name: websearch-api\n    restart: unless-stopped\n    ports:\n      - \"3001:3001\"\n    environment:\n      - NODE_ENV=production\n      - PORT=3001\n      - LOG_LEVEL=info\n      - FLARESOLVERR_URL=http://flaresolverr:8191/v1\n    depends_on:\n      - flaresolverr\n    volumes:\n      - crawler_storage:/app/storage\n\n  flaresolverr:\n    image: 21hsmw/flaresolverr:nodriver\n    container_name: flaresolverr\n    restart: unless-stopped\n    environment:\n      - LOG_LEVEL=info\n      - TZ=UTC\n\nvolumes:\n  crawler_storage:\n```\nworkaround for Mac Apple Silicon\n```\nversion: '3.8'\n\nservices:\n  crawler:\n    image: laituanmanh/websearch-crawler:latest\n    container_name: websearch-api\n    platform: \"linux/amd64\"\n    restart: unless-stopped\n    ports:\n      - \"3001:3001\"\n    environment:\n      - NODE_ENV=production\n      - PORT=3001\n      - LOG_LEVEL=info\n      - FLARESOLVERR_URL=http://flaresolverr:8191/v1\n    depends_on:\n      - flaresolverr\n    volumes:\n      - crawler_storage:/app/storage\n\n  flaresolverr:\n    image: 21hsmw/flaresolverr:nodriver\n    platform: \"linux/arm64\"\n    container_name: flaresolverr\n    restart: unless-stopped\n    environment:\n      - LOG_LEVEL=info\n      - TZ=UTC\n\nvolumes:\n  crawler_storage:\n```\n\n2. Start the services:\n\n```bash\ndocker-compose up -d\n```\n\n3. Verify that the services are running:\n\n```bash\ndocker-compose ps\n```\n\n4. Test the crawler API health endpoint:\n\n```bash\ncurl http://localhost:3001/health\n```\n\nExpected response:\n```json\n{\n  \"status\": \"ok\",\n  \"details\": {\n    \"status\": \"ok\",\n    \"flaresolverr\": true,\n    \"google\": true,\n    \"message\": null\n  }\n}\n```\n\nThe crawler API will be available at `http://localhost:3001`.\n\n### Testing the Crawler API\n\nYou can test the crawler API directly using curl:\n\n```bash\ncurl -X POST http://localhost:3001/crawl \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": \"typescript best practices\",\n    \"numResults\": 2,\n    \"language\": \"en\",\n    \"filters\": {\n      \"excludeDomains\": [\"youtube.com\"],\n      \"resultType\": \"all\" \n    }\n  }'\n```\n\n### Custom Configuration\n\nYou can customize the crawler service by modifying the environment variables in the `docker-compose.yml` file:\n\n- `PORT`: The port on which the crawler API listens (default: 3001)\n- `LOG_LEVEL`: Logging level (options: debug, info, warn, error)\n- `FLARESOLVERR_URL`: URL of the FlareSolverr service (for bypassing Cloudflare protection)\n\n## Integrating with MCP Clients\n\n### Quick Reference: MCP Configuration\n\nHere's a quick reference for MCP configuration across different clients:\n\n```json\n{\n    \"mcpServers\": {\n        \"websearch\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"websearch-mcp\"\n            ],\n            \"environment\": {\n                \"API_URL\": \"http://localhost:3001\",\n                \"MAX_SEARCH_RESULT\": \"5\" // reduce to save your tokens, increase for wider information gain\n            }\n        }\n    }\n}\n```\n\nWorkaround for Windows, due to [Issue](https://github.com/smithery-ai/mcp-obsidian/issues/19)\n```\n{\n\t\"mcpServers\": {\n\t  \"websearch\": {\n            \"command\": \"cmd\",\n            \"args\": [\n\t\t\t\t\"/c\",\n\t\t\t\t\"npx\",\n                \"websearch-mcp\"\n            ],\n            \"environment\": {\n                \"API_URL\": \"http://localhost:3001\",\n                \"MAX_SEARCH_RESULT\": \"1\"\n            }\n        }\n\t}\n  }\n```\n\n## Usage\n\nThis package implements an MCP server using stdio transport that exposes a `web_search` tool with the following parameters:\n\n### Parameters\n\n- `query` (required): The search query to look up\n- `numResults` (optional): Number of results to return (default: 5)\n- `language` (optional): Language code for search results (e.g., 'en')\n- `region` (optional): Region code for search results (e.g., 'us')\n- `excludeDomains` (optional): Domains to exclude from results\n- `includeDomains` (optional): Only include these domains in results\n- `excludeTerms` (optional): Terms to exclude from results\n- `resultType` (optional): Type of results to return ('all', 'news', or 'blogs')\n\n### Example Search Response\n\nHere's an example of a search response:\n\n```json\n{\n  \"query\": \"machine learning trends\",\n  \"results\": [\n    {\n      \"title\": \"Top Machine Learning Trends in 2025\",\n      \"snippet\": \"The key machine learning trends for 2025 include multimodal AI, generative models, and quantum machine learning applications in enterprise...\",\n      \"url\": \"https://example.com/machine-learning-trends-2025\",\n      \"siteName\": \"AI Research Today\",\n      \"byline\": \"Dr. Jane Smith\"\n    },\n    {\n      \"title\": \"The Evolution of Machine Learning: 2020-2025\",\n      \"snippet\": \"Over the past five years, machine learning has evolved from primarily supervised learning approaches to more sophisticated self-supervised and reinforcement learning paradigms...\",\n      \"url\": \"https://example.com/ml-evolution\",\n      \"siteName\": \"Tech Insights\",\n      \"byline\": \"John Doe\"\n    }\n  ]\n}\n```\n\n### Testing Locally\n\nTo test the WebSearch MCP server locally, you can use the included test client:\n\n```bash\nnpm run test-client\n```\n\nThis will start the MCP server and a simple command-line interface that allows you to enter search queries and see the results.\n\nYou can also configure the API_URL for the test client:\n\n```bash\nAPI_URL=https://crawler.example.com npm run test-client\n```\n\n### As a Library\n\nYou can use this package programmatically:\n\n```typescript\nimport { createMCPClient } from '@modelcontextprotocol/sdk';\n\n// Create an MCP client\nconst client = createMCPClient({\n  transport: { type: 'subprocess', command: 'npx websearch-mcp' }\n});\n\n// Execute a web search\nconst response = await client.request({\n  method: 'call_tool',\n  params: {\n    name: 'web_search',\n    arguments: {\n      query: 'your search query',\n      numResults: 5,\n      language: 'en'\n    }\n  }\n});\n\nconsole.log(response.result);\n```\n\n## Troubleshooting\n\n### Crawler Service Issues\n\n- **API Unreachable**: Ensure that the crawler service is running and accessible at the configured API_URL.\n- **Search Results Not Available**: Check the logs of the crawler service to see if there are any errors:\n  ```bash\n  docker-compose logs crawler\n  ```\n- **FlareSolverr Issues**: Some websites use Cloudflare protection. If you see errors related to this, check if FlareSolverr is working:\n  ```bash\n  docker-compose logs flaresolverr\n  ```\n\n### MCP Server Issues\n\n- **Import Errors**: Ensure you have the latest version of the MCP SDK:\n  ```bash\n  npm install -g @modelcontextprotocol/sdk@latest\n  ```\n- **Connection Issues**: Make sure the stdio transport is properly configured for your client.\n\n## Development\n\nTo work on this project:\n\n1. Clone the repository\n2. Install dependencies: `npm install`\n3. Build the project: `npm run build`\n4. Run in development mode: `npm run dev`\n\nThe server expects a WebSearch Crawler API as defined in the included swagger.json file. Make sure the API is running at the configured API_URL.\n\n### Project Structure\n\n- `.gitignore`: Specifies files that Git should ignore (node_modules, dist, logs, etc.)\n- `.npmignore`: Specifies files that shouldn't be included when publishing to npm\n- `package.json`: Project metadata and dependencies\n- `src/`: Source TypeScript files\n- `dist/`: Compiled JavaScript files (generated when building)\n\n### Publishing to npm\n\nTo publish this package to npm:\n\n1. Make sure you have an npm account and are logged in (`npm login`)\n2. Update the version in package.json (`npm version patch|minor|major`)\n3. Run `npm publish`\n\nThe `.npmignore` file ensures that only the necessary files are included in the published package:\n- The compiled code in `dist/`\n- README.md and LICENSE files\n- package.json\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nISC\n"
}