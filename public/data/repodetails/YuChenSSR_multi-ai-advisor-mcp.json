{
  "mcp_name": "YuChenSSR/multi-ai-advisor",
  "mcp_description": "üìá üè† - A Model Context Protocol (MCP) server that queries multiple Ollama models and combines their responses, providing diverse AI perspectives on a single question.",
  "mcp_id": "YuChenSSR_multi-ai-advisor-mcp",
  "fetch_timestamp": "2025-06-23T09:43:19.044948Z",
  "github_url": "https://github.com/YuChenSSR/multi-ai-advisor-mcp",
  "repository": {
    "name": "multi-ai-advisor-mcp",
    "full_name": "YuChenSSR/multi-ai-advisor-mcp",
    "description": "council of models for decision",
    "html_url": "https://github.com/YuChenSSR/multi-ai-advisor-mcp",
    "created_at": "2025-03-24T12:52:06Z",
    "updated_at": "2025-06-10T19:55:27Z",
    "pushed_at": "2025-04-02T19:12:18Z",
    "size": 6544,
    "stargazers_count": 49,
    "watchers_count": 49,
    "forks_count": 15,
    "open_issues_count": 1,
    "language": "TypeScript",
    "license": "MIT License",
    "topics": [
      "ai-communication",
      "ollama"
    ],
    "default_branch": "main",
    "owner": {
      "login": "YuChenSSR",
      "type": "User",
      "avatar_url": "https://avatars.githubusercontent.com/u/55662670?v=4",
      "html_url": "https://github.com/YuChenSSR"
    },
    "has_issues": true,
    "has_projects": true,
    "has_downloads": true,
    "has_wiki": true,
    "has_pages": false,
    "archived": false,
    "disabled": false,
    "visibility": "public",
    "network_count": 15,
    "subscribers_count": 1,
    "languages": {
      "TypeScript": 9066,
      "Dockerfile": 459
    },
    "language_percentages": {
      "TypeScript": 95.18,
      "Dockerfile": 4.82
    },
    "pull_requests_count": 2,
    "contributors_count": 3,
    "package_json_version": "1.0.0"
  },
  "readme": "# Multi-Model Advisor\n## (ÈîµÈîµÂõõ‰∫∫Ë°å)\n\n[![smithery badge](https://smithery.ai/badge/@YuChenSSR/multi-ai-advisor-mcp)](https://smithery.ai/server/@YuChenSSR/multi-ai-advisor-mcp)\n\nA Model Context Protocol (MCP) server that queries multiple Ollama models and combines their responses, providing diverse AI perspectives on a single question. This creates a \"council of advisors\" approach where Claude can synthesize multiple viewpoints alongside its own to provide more comprehensive answers.\n\n<a href=\"https://glama.ai/mcp/servers/@YuChenSSR/multi-ai-advisor-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@YuChenSSR/multi-ai-advisor-mcp/badge\" alt=\"Multi-Model Advisor MCP server\" />\n</a>\n\n```mermaid\ngraph TD\n    A[Start] --> B[Worker Local AI 1 Opinion]\n    A --> C[Worker Local AI 2 Opinion]\n    A --> D[Worker Local AI 3 Opinion]\n    B --> E[Manager AI]\n    C --> E\n    D --> E\n    E --> F[Decision Made]\n```\n\n## Features\n\n- Query multiple Ollama models with a single question\n- Assign different roles/personas to each model\n- View all available Ollama models on your system\n- Customize system prompts for each model\n- Configure via environment variables\n- Integrate seamlessly with Claude for Desktop\n\n## Prerequisites\n\n- Node.js 16.x or higher\n- Ollama installed and running (see [Ollama installation](https://github.com/ollama/ollama#installation))\n- Claude for Desktop (for the complete advisory experience)\n\n## Installation\n\n### Installing via Smithery\n\nTo install multi-ai-advisor-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@YuChenSSR/multi-ai-advisor-mcp):\n\n```bash\nnpx -y @smithery/cli install @YuChenSSR/multi-ai-advisor-mcp --client claude\n```\n\n### Manual Installation\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/YuChenSSR/multi-ai-advisor-mcp.git \n   cd multi-ai-advisor-mcp\n   ```\n\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n\n3. Build the project:\n   ```bash\n   npm run build\n   ```\n\n4. Install required Ollama models:\n   ```bash\n   ollama pull gemma3:1b\n   ollama pull llama3.2:1b\n   ollama pull deepseek-r1:1.5b\n   ```\n\n## Configuration\n\nCreate a `.env` file in the project root with your desired configuration:\n\n```\n# Server configuration\nSERVER_NAME=multi-model-advisor\nSERVER_VERSION=1.0.0\nDEBUG=true\n\n# Ollama configuration\nOLLAMA_API_URL=http://localhost:11434\nDEFAULT_MODELS=gemma3:1b,llama3.2:1b,deepseek-r1:1.5b\n\n# System prompts for each model\nGEMMA_SYSTEM_PROMPT=You are a creative and innovative AI assistant. Think outside the box and offer novel perspectives.\nLLAMA_SYSTEM_PROMPT=You are a supportive and empathetic AI assistant focused on human well-being. Provide considerate and balanced advice.\nDEEPSEEK_SYSTEM_PROMPT=You are a logical and analytical AI assistant. Think step-by-step and explain your reasoning clearly.\n```\n\n## Connect to Claude for Desktop\n\n1. Locate your Claude for Desktop configuration file:\n   - MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n   - Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n2. Edit the file to add the Multi-Model Advisor MCP server:\n\n```json\n{\n  \"mcpServers\": {\n    \"multi-model-advisor\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/multi-ai-advisor-mcp/build/index.js\"]\n    }\n  }\n}\n```\n\n3. Replace `/absolute/path/to/` with the actual path to your project directory\n\n4. Restart Claude for Desktop\n\n## Usage\n\nOnce connected to Claude for Desktop, you can use the Multi-Model Advisor in several ways:\n\n### List Available Models\n\nYou can see all available models on your system:\n\n```\nShow me which Ollama models are available on my system\n```\n\nThis will display all installed Ollama models and indicate which ones are configured as defaults.\n\n### Basic Usage\n\nSimply ask Claude to use the multi-model advisor:\n\n```\nwhat are the most important skills for success in today's job market, \nyou can use gemma3:1b, llama3.2:1b, deepseek-r1:1.5b to help you \n```\n\nClaude will query all default models and provide a synthesized response based on their different perspectives.\n\n![example](https://raw.githubusercontent.com/YuChenSSR/pics/master/imgs/2025-03-24/Q53YEwdTaeTuL6a7.png)\n\n\n\n## How It Works\n\n1. The MCP server exposes two tools:\n   - `list-available-models`: Shows all Ollama models on your system\n   - `query-models`: Queries multiple models with a question\n\n2. When you ask Claude a question referring to the multi-model advisor:\n   - Claude decides to use the `query-models` tool\n   - The server sends your question to multiple Ollama models\n   - Each model responds with its perspective\n   - Claude receives all responses and synthesizes a comprehensive answer\n\n3. Each model can have a different \"persona\" or role assigned, encouraging diverse perspectives.\n\n## Troubleshooting\n\n### Ollama Connection Issues\n\nIf the server can't connect to Ollama:\n- Ensure Ollama is running (`ollama serve`)\n- Check that the OLLAMA_API_URL is correct in your .env file\n- Try accessing http://localhost:11434 in your browser to verify Ollama is responding\n\n### Model Not Found\n\nIf a model is reported as unavailable:\n- Check that you've pulled the model using `ollama pull <model-name>`\n- Verify the exact model name using `ollama list`\n- Use the `list-available-models` tool to see all available models\n\n### Claude Not Showing MCP Tools\n\nIf the tools don't appear in Claude:\n- Ensure you've restarted Claude after updating the configuration\n- Check the absolute path in claude_desktop_config.json is correct\n- Look at Claude's logs for error messages\n\n### RAM is not enough\n\nSome managers' AI models may have chosen larger models, but there is not enough memory to run them. You can try specifying a smaller model (see the [Basic Usage](#basic-usage)) or upgrading the memory.\n\n## License\n\nMIT License\n\nFor more details, please see the LICENSE file in [this project repository](https://github.com/YuChenSSR/multi-ai-advisor-mcp)\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request."
}