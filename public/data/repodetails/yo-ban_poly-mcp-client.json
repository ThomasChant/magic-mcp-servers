{
  "mcp_name": "yo-ban/poly-mcp-client",
  "mcp_description": "A Python client library for managing connections to MCP servers, adapting tool definitions for Anthropic, OpenAI, and Gemini.",
  "mcp_id": "yo-ban_poly-mcp-client",
  "fetch_timestamp": "2025-06-23T09:40:01.695693Z",
  "github_url": "https://github.com/yo-ban/poly-mcp-client",
  "repository": {
    "name": "poly-mcp-client",
    "full_name": "yo-ban/poly-mcp-client",
    "description": "An simple MCP client adapting tool definitions for Anthropic, OpenAI, and Gemini.",
    "html_url": "https://github.com/yo-ban/poly-mcp-client",
    "created_at": "2025-04-21T10:42:36Z",
    "updated_at": "2025-05-25T02:41:44Z",
    "pushed_at": "2025-05-25T02:41:41Z",
    "size": 57,
    "stargazers_count": 1,
    "watchers_count": 1,
    "forks_count": 0,
    "open_issues_count": 0,
    "language": "Python",
    "license": "MIT License",
    "topics": [
      "mcp",
      "mcp-client"
    ],
    "default_branch": "main",
    "owner": {
      "login": "yo-ban",
      "type": "User",
      "avatar_url": "https://avatars.githubusercontent.com/u/4954078?v=4",
      "html_url": "https://github.com/yo-ban"
    },
    "has_issues": true,
    "has_projects": true,
    "has_downloads": true,
    "has_wiki": true,
    "has_pages": false,
    "archived": false,
    "disabled": false,
    "visibility": "public",
    "network_count": 0,
    "subscribers_count": 1,
    "languages": {
      "Python": 81882
    },
    "language_percentages": {
      "Python": 100
    },
    "pull_requests_count": 0,
    "contributors_count": 1
  },
  "readme": "# poly-mcp-client\r\n\r\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\r\n\r\nA simple Python client library for managing connections to MCP (Model Context Protocol) servers, built on top of the official [mcp-sdk](https://github.com/modelcontextprotocol/python-sdk). It adapts tool definitions for use with different LLM vendors like Anthropic, OpenAI, and Gemini.\r\n\r\n## Features\r\n\r\n*   **Vendor-Agnostic Tool Handling:**\r\n    *   Retrieve available tool definitions in a Canonical format.\r\n    *   Convert tool definitions on-the-fly to formats compatible with **Anthropic**, **OpenAI**, and **Gemini**.\r\n*   **Prefixed Naming:** Automatically prefixes tool names and resource URIs with `mcp-{server_name}-` (customizable). This allows handling servers that might expose tools/resources with the same original name without conflicts.\r\n*   **Flexible Configuration:** Define MCP servers using a JSON configuration file similar to Claude Desktop's `claude_desktop_config.json`.\r\n\r\n## Status\r\n\r\n### Version 0.0.1\r\n\r\n*   Core functionality (tool listing, conversion, execution) is implemented.\r\n*   Resource listing and reading are implemented but needs further testing.\r\n*   SSE transport support is implemented but needs further testing.\r\n*   Prompt-related features are not yet implemented.\r\n*   This is an early alpha version. APIs might change. Use with caution.\r\n\r\n### Version 0.0.4\r\n*   Streamable-HTTP Transport Support (Stateless Only).\r\n\r\n\r\n## Installation\r\n\r\n```bash\r\ngit clone https://github.com/your-username/poly-mcp-client.git\r\ncd poly-mcp-client\r\npip install .\r\n```\r\n\r\n## Configuration\r\n\r\nCreate a JSON file (e.g., `mcp_servers.json`) to define your MCP servers. The structure is similar to Claude Desktop's configuration:\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"filesystem\": {\r\n      \"type\": \"stdio\",\r\n      \"command\": \"npx\",\r\n      \"args\": [\r\n        \"-y\",\r\n        \"@modelcontextprotocol/server-filesystem\",\r\n        \"/path/to/your/accessible/directory\"\r\n      ],\r\n      \"env\": null\r\n    },\r\n    \"sequential-thinking\": {\r\n      \"type\": \"stdio\",\r\n      \"command\": \"npx\",\r\n      \"args\": [\r\n          \"-y\", \"@modelcontextprotocol/server-sequential-thinking\"\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\n**Note:** Ensure the `command` and `args` are correct for your environment and server setup. Use absolute paths where necessary. You'll also need `npx` (usually installed with Node.js) for this example config.\r\n\r\n## Basic Usage\r\n\r\nThis example shows how to get tools from MCP servers managed by `PolyMCPClient` and use them.\r\n\r\n### Anthropic\r\n\r\n```python\r\nimport asyncio\r\nimport logging\r\nimport json\r\nimport os\r\nfrom poly_mcp_client import PolyMCPClient # Use the correct class name\r\nfrom anthropic import Anthropic # Import Anthropic library\r\nfrom dotenv import load_dotenv\r\n\r\nload_dotenv()\r\n\r\n# Setup logging (optional but recommended)\r\nlogging.basicConfig(level=logging.INFO)\r\n# You might want finer control, e.g., logging only poly_mcp_client messages\r\n# logging.getLogger(\"poly_mcp_client\").setLevel(logging.DEBUG)\r\n\r\n# --- Anthropic API Key ---\r\n# Make sure your ANTHROPIC_API_KEY environment variable is set\r\n# e.g., export ANTHROPIC_API_KEY='your-api-key'\r\nanthropic_client = Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\")) # client() initializes from ANTHROPIC_API_KEY env var\r\n\r\nasync def main():\r\n    # Initialize the PolyMCPClient\r\n    mcp_client = PolyMCPClient() # Default prefix is \"mcp-\"\r\n    await mcp_client.initialize(config_path=\"./mcp_servers.json\") # Adjust path if needed\r\n\r\n    # Wait for initial connections\r\n    print(\"Waiting for initial connections...\")\r\n    connection_results = await mcp_client.wait_for_initial_connections(timeout=45.0)\r\n    print(\"Initial Connection Results:\", connection_results)\r\n\r\n    # Check if any servers successfully connected\r\n    successful_connections = [name for name, (success, _) in connection_results.items() if success]\r\n    if not successful_connections:\r\n        print(\"No MCP servers connected successfully. Exiting.\")\r\n        await mcp_client.shutdown()\r\n        return\r\n\r\n    print(f\"Successfully connected to: {successful_connections}\")\r\n\r\n    try:\r\n        # 1. Get available tools in Anthropic format\r\n        print(\"\\nFetching tools for Anthropic...\")\r\n        anthropic_tools = await mcp_client.get_available_tools(vendor=\"anthropic\")\r\n        print(f\"Found {len(anthropic_tools)} tools (Anthropic format).\")\r\n\r\n        # 2. Define the user's message\r\n        # Example: Ask Claude to list files on the Desktop using the filesystem server\r\n        user_message = \"Can you list all the files in my folder?\"\r\n\r\n        messages = [{\"role\": \"user\", \"content\": user_message}]\r\n\r\n        print(f\"\\nSending request to Claude with {len(anthropic_tools)} tools...\")\r\n\r\n        # 3. Call Anthropic API (initial request)\r\n        response = anthropic_client.messages.create(\r\n            model=\"claude-3-7-sonnet-20250219\", # Or your preferred model\r\n            max_tokens=1024,\r\n            messages=messages,\r\n            tools=anthropic_tools,\r\n        )\r\n\r\n        print(\"\\nClaude's initial response type:\", response.stop_reason)\r\n\r\n        # 4. Process the response - Check for tool use\r\n        while response.stop_reason == \"tool_use\":\r\n            tool_results = []\r\n            for content_block in response.content:\r\n                if content_block.type == \"tool_use\":\r\n                    tool_name = content_block.name\r\n                    tool_input = content_block.input\r\n                    tool_use_id = content_block.id\r\n\r\n                    print(f\"\\nClaude wants to use tool: {tool_name}\")\r\n                    print(f\"With input: {tool_input}\")\r\n\r\n                    # 5. Execute the tool using PolyMCPClient\r\n                    try:\r\n                        tool_output_content = await mcp_client.execute_mcp_tool(\r\n                            full_tool_name=tool_name,\r\n                            arguments=tool_input\r\n                        )\r\n                        print(f\"Tool '{tool_name}' executed successfully.\")\r\n\r\n                        # 6. Format the result for Anthropic\r\n                        # The 'tool_output_content' received from 'execute_mcp_tool' is a list\r\n                        # of content items (like TextContent, ImageContent). Conveniently,\r\n                        # the MCP SDK's structure for this list is designed to be directly\r\n                        # compatible with the list format expected by Anthropic's 'tool_result'\r\n                        # content block.\r\n                        # Therefore, for Anthropic, we can pass 'tool_output_content' directly.\r\n                        # If using a different vendor (like OpenAI or Gemini), you would need\r\n                        # to convert 'tool_output_content' into that vendor's specific\r\n                        # required format here (e.g., a single JSON string for OpenAI function calls).\r\n\r\n                        tool_results.append({\r\n                            \"type\": \"tool_result\",\r\n                            \"tool_use_id\": tool_use_id,\r\n                            \"content\": tool_output_content, # Pass the list directly for Anthropic\r\n                            # Note: If execute_mcp_tool raised an error handled below,\r\n                            # this 'content' will be overwritten by the error message content.\r\n                        })\r\n\r\n                    except (ValueError, ConnectionError, RuntimeError, TimeoutError, Exception) as e:\r\n                        print(f\"Error executing tool '{tool_name}': {e}\")\r\n                        # Report the error back to Claude\r\n                        tool_results.append({\r\n                            \"type\": \"tool_result\",\r\n                            \"tool_use_id\": tool_use_id,\r\n                            \"content\": f\"Error executing tool: {str(e)}\",\r\n                        })\r\n\r\n            # If there were tool results, add them to the message history\r\n            if tool_results:\r\n                messages.append({\"role\": \"assistant\", \"content\": response.content}) # Add Claude's turn\r\n                messages.append({\"role\": \"user\", \"content\": tool_results}) # Add the tool results\r\n\r\n                # 7. Call Anthropic API again with the tool results\r\n                print(\"\\nSending tool results back to Claude...\")\r\n                response = anthropic_client.messages.create(\r\n                    model=\"claude-3-5-sonnet-20240620\",\r\n                    max_tokens=1024,\r\n                    messages=messages,\r\n                    tools=anthropic_tools,\r\n                )\r\n                print(\"\\nClaude's response type after tool execution:\", response.stop_reason)\r\n            else:\r\n                # Should not happen if stop_reason was tool_use, but break defensively\r\n                print(\"Warning: stop_reason was 'tool_use' but no tool results generated.\")\r\n                break\r\n\r\n\r\n        # 8. Handle the final response from Claude\r\n        print(\"\\nClaude's Final Response:\")\r\n        final_text = \"\"\r\n        for content_block in response.content:\r\n            if content_block.type == \"text\":\r\n                print(content_block.text)\r\n                final_text += content_block.text + \"\\n\"\r\n        \r\n        if not final_text:\r\n            print(\"(No text content in final response)\")\r\n            print(\"Full final response object:\", response)\r\n\r\n\r\n    except Exception as e:\r\n        print(f\"\\nAn unexpected error occurred: {e}\")\r\n        logging.exception(\"Error during main execution:\") # Log stack trace\r\n    finally:\r\n        # Shutdown the client gracefully\r\n        print(\"\\nShutting down PolyMCPClient...\")\r\n        await mcp_client.shutdown()\r\n        print(\"Shutdown complete.\")\r\n\r\nif __name__ == \"__main__\":\r\n    # Example of running the async main function\r\n    # Ensure you run this in an environment where top-level await is supported\r\n    # or use asyncio.run()\r\n    try:\r\n        asyncio.run(main())\r\n    except KeyboardInterrupt:\r\n        print(\"\\nInterrupted by user.\")\r\n```\r\n\r\n### Gemini (Google AI Studio)\r\n\r\n```python\r\nimport asyncio\r\nimport logging\r\nimport os\r\nfrom poly_mcp_client import PolyMCPClient\r\nfrom dotenv import load_dotenv\r\nfrom google import genai\r\nfrom google.genai.types import (\r\n    ToolConfig,\r\n    FunctionCallingConfig,\r\n    GenerateContentConfig,\r\n    Content,\r\n    Part,\r\n    Tool,\r\n    FunctionDeclaration,\r\n)\r\n\r\n# Load environment variables (for API key)\r\nload_dotenv()\r\n\r\n# Setup logging (optional)\r\nlogging.basicConfig(level=logging.INFO)\r\n\r\n# --- Google API Key ---\r\n# Make sure your GEMINI_API_KEY environment variable is set\r\n# e.g., export GEMINI_API_KEY='your-api-key'\r\ngenai_client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\r\n\r\nasync def main():\r\n    # Initialize the PolyMCPClient\r\n    mcp_client = PolyMCPClient() # Default prefix is \"mcp-\"\r\n    await mcp_client.initialize(config_path=\"./mcp_servers.json\")\r\n\r\n    # Wait for initial connections (important!)\r\n    print(\"Waiting for initial connections...\")\r\n    connection_results = await mcp_client.wait_for_initial_connections(timeout=45.0)\r\n    print(\"Initial Connection Results:\", connection_results)\r\n\r\n    # Check if any servers successfully connected\r\n    successful_connections = [name for name, (success, _) in connection_results.items() if success]\r\n    if not successful_connections:\r\n        print(\"No MCP servers connected successfully. Exiting.\")\r\n        await mcp_client.shutdown()\r\n        return\r\n\r\n    print(f\"Successfully connected to: {successful_connections}\")\r\n\r\n    try:\r\n        # 1. Get available tools in Gemini format\r\n        print(\"\\nFetching tools for Google Gemini...\")\r\n        # The get_available_tools method returns dictionaries suitable for FunctionDeclaration\r\n        gemini_tool_dicts = await mcp_client.get_available_tools(vendor=\"google\")\r\n        print(f\"Found {len(gemini_tool_dicts)} tools (Gemini format).\")\r\n\r\n        # Prepare the tools for the API call\r\n        if gemini_tool_dicts:\r\n            gemini_tools = [Tool(function_declarations=[FunctionDeclaration(**tool) for tool in gemini_tool_dicts])]\r\n            tool_config = ToolConfig(\r\n                function_calling_config=FunctionCallingConfig(mode='AUTO') # Mode 'ANY': Allows the model to predict a function call on any turn.\r\n            )\r\n        else:\r\n            gemini_tools = []\r\n            tool_config = None # No tools, no tool config needed\r\n\r\n\r\n        # 2. Define the user's message\r\n        # Adjust the user message based on your configured server and path\r\n        user_message = \"Can you list all the files in my folder? path/to/folder\"\r\n\r\n        # 3. Initialize chat history\r\n        history = []\r\n        latest_content = Content(parts=[Part.from_text(text=user_message)], role=\"user\")\r\n        history.append(latest_content) # Add initial user message to history\r\n\r\n        print(f\"\\nSending request to Gemini with {len(gemini_tool_dicts)} tools...\")\r\n\r\n        while True: # Keep interacting until the model responds with text\r\n            # 4. Prepare generation config for the API call\r\n            generation_config = GenerateContentConfig(\r\n                temperature=0.7, # Adjust generation parameters as needed\r\n                max_output_tokens=1024,\r\n                tools=gemini_tools if gemini_tools else None, # Pass tools if available\r\n                tool_config=tool_config if tool_config else None, # Pass tool_config if available\r\n            )\r\n\r\n            # 5. Call Google Gemini API\r\n            response = await genai_client.aio.models.generate_content(\r\n                model=\"gemini-2.0-flash\", # Use appropriate model name\r\n                contents=history, # Pass the full chat history\r\n                config=generation_config\r\n            )\r\n\r\n            # 6. Process the response - Check for function call or text\r\n            candidate = response.candidates[0] # Get the first candidate\r\n            first_part = candidate.content.parts[0] if candidate.content.parts else None\r\n\r\n            if first_part and hasattr(first_part, 'function_call') and first_part.function_call:\r\n                function_call = first_part.function_call\r\n                print(f\"\\nGemini wants to use tool: {function_call.name}\")\r\n                print(f\"With input: {function_call.args}\")\r\n\r\n                # Add the model's request to use the function to history\r\n                model_turn_parts = [Part.from_function_call(name=function_call.name, args=function_call.args)]\r\n                history.append(Content(role=\"model\", parts=model_turn_parts))\r\n\r\n                # 7. Execute the tool using PolyMCPClient\r\n                try:\r\n                    # Arguments are expected as a dict by execute_mcp_tool\r\n                    tool_args_dict = dict(function_call.args.items()) if function_call.args else {}\r\n                    tool_output_content = await mcp_client.execute_mcp_tool(\r\n                        full_tool_name=function_call.name,\r\n                        arguments=tool_args_dict\r\n                    )\r\n                    print(f\"Tool '{function_call.name}' executed successfully.\")\r\n\r\n                    # 8. Format the result for Gemini\r\n                    # Convert the list of MCP content items (TextContent, etc.)\r\n                    # into a list of Gemini Part objects using Part.from_function_response.\r\n                    function_response_parts = []\r\n                    for mcp_item in tool_output_content:\r\n                        # Gemini's function response 'response' field expects a dict.\r\n                        # We wrap the MCP item's details (converted to dict) here.\r\n                        # Adjust this logic if tools return non-text content (e.g., images).\r\n                        if hasattr(mcp_item, 'text'):\r\n                            # Pass the MCP item's dictionary representation\r\n                            response_data = {\"content\": mcp_item.model_dump()}\r\n                            function_response_parts.append(\r\n                                Part.from_function_response(\r\n                                    name=function_call.name,\r\n                                    response=response_data\r\n                                )\r\n                            )\r\n                        # else: Handle other MCP content types (ImageContent, etc.) if needed\r\n\r\n                    if not function_response_parts:\r\n                        # Handle cases where the tool executes but has no suitable output for Gemini\r\n                        print(f\"Warning: Tool '{function_call.name}' executed but produced no convertible output for Gemini.\")\r\n                        function_response_parts.append(\r\n                            Part.from_function_response(\r\n                                name=function_call.name,\r\n                                response={\"content\": \"Tool executed successfully, but no standard output provided.\"}\r\n                            )\r\n                        )\r\n\r\n                    # Add the function execution result to history (as 'user' role according to Gemini docs)\r\n                    history.append(Content(role=\"user\", parts=function_response_parts))\r\n                    print(\"\\nSending tool results back to Gemini...\")\r\n                    # Loop continues to call generate_content again\r\n\r\n                    tool_config = ToolConfig(\r\n                        function_calling_config=FunctionCallingConfig(mode='AUTO') # Mode 'AUTO': model is automatically deciding whether to use a function call or not.\r\n                    )\r\n\r\n                except (ValueError, ConnectionError, RuntimeError, TimeoutError, Exception) as e:\r\n                    print(f\"Error executing tool '{function_call.name}': {e}\")\r\n                    # Report the error back to Gemini\r\n                    error_response_part = Part.from_function_response(\r\n                            name=function_call.name,\r\n                            # Provide error information in the response content\r\n                            response={\"error\": f\"Failed to execute tool: {str(e)}\"}\r\n                        )\r\n                    # Add error result to history\r\n                    history.append(Content(role=\"user\", parts=[error_response_part]))\r\n                    print(\"\\nSending error information back to Gemini...\")\r\n                    # Loop continues to call generate_content again\r\n\r\n            elif first_part and hasattr(first_part, 'text'):\r\n                # 9. Handle the final text response from Gemini\r\n                print(\"\\nGemini's Final Response:\")\r\n                final_text = \"\"\r\n                # Iterate through all parts in the final response in case of multiple text parts\r\n                for part in candidate.content.parts:\r\n                    if hasattr(part, 'text'):\r\n                        print(part.text)\r\n                        final_text += part.text + \"\\n\"\r\n\r\n                if not final_text:\r\n                    print(\"(No text content in final response)\")\r\n                    # print(\"Full final response object:\", response) # For debugging\r\n\r\n                # Add final model response to history (optional, if continuing chat)\r\n                history.append(candidate.content)\r\n                break # Exit the loop as we have a final text response\r\n\r\n            else:\r\n                # Handle unexpected response content\r\n                print(\"\\nReceived unexpected response content from Gemini:\")\r\n                print(response)\r\n                break # Exit loop on unexpected response\r\n\r\n    except Exception as e:\r\n        print(f\"\\nAn unexpected error occurred: {e}\")\r\n        logging.exception(\"Error during main execution:\") # Log stack trace\r\n    finally:\r\n        # Shutdown the client gracefully\r\n        print(\"\\nShutting down PolyMCPClient...\")\r\n        await mcp_client.shutdown()\r\n        print(\"Shutdown complete.\")\r\n\r\nif __name__ == \"__main__\":\r\n    try:\r\n        asyncio.run(main())\r\n    except KeyboardInterrupt:\r\n        print(\"\\nInterrupted by user.\")\r\n```\r\n\r\n\r\n## Dependencies\r\n\r\n*   [mcp-sdk](https://github.com/modelcontextprotocol/python-sdk)\r\n*   [pydantic](https://docs.pydantic.dev/)\r\n\r\n## License\r\n\r\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\r\n"
}