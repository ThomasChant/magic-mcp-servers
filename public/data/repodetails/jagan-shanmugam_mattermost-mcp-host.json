{
  "mcp_name": "jagan-shanmugam/mattermost-mcp-host",
  "mcp_description": "üêç üè† - A MCP server along with MCP host that provides access to Mattermost teams, channels and messages. MCP host is integrated as a bot in Mattermost with access to MCP servers that can be configured.",
  "mcp_id": "jagan-shanmugam_mattermost-mcp-host",
  "fetch_timestamp": "2025-06-23T05:06:42.512736Z",
  "github_url": "https://github.com/jagan-shanmugam/mattermost-mcp-host",
  "repository": {
    "name": "mattermost-mcp-host",
    "full_name": "jagan-shanmugam/mattermost-mcp-host",
    "description": "A Mattermost integration that connects to Model Context Protocol (MCP) servers, leveraging a LangGraph-based Agent.",
    "html_url": "https://github.com/jagan-shanmugam/mattermost-mcp-host",
    "created_at": "2025-03-03T18:38:07Z",
    "updated_at": "2025-06-19T20:02:11Z",
    "pushed_at": "2025-04-07T19:56:58Z",
    "size": 125449,
    "stargazers_count": 18,
    "watchers_count": 18,
    "forks_count": 11,
    "open_issues_count": 4,
    "language": "Python",
    "license": "MIT License",
    "topics": [
      "langgraph",
      "llm",
      "mattermost",
      "mcp",
      "mcp-clients"
    ],
    "default_branch": "main",
    "owner": {
      "login": "jagan-shanmugam",
      "type": "User",
      "avatar_url": "https://avatars.githubusercontent.com/u/30863630?v=4",
      "html_url": "https://github.com/jagan-shanmugam"
    },
    "has_issues": true,
    "has_projects": true,
    "has_downloads": true,
    "has_wiki": true,
    "has_pages": false,
    "archived": false,
    "disabled": false,
    "visibility": "public",
    "network_count": 11,
    "subscribers_count": 2,
    "languages": {
      "Python": 137767,
      "Dockerfile": 375
    },
    "language_percentages": {
      "Python": 99.73,
      "Dockerfile": 0.27
    },
    "pull_requests_count": 1,
    "contributors_count": 1
  },
  "readme": "# Mattermost MCP Host\n\nA Mattermost integration that connects to Model Context Protocol (MCP) servers, leveraging a LangGraph-based AI agent to provide an intelligent interface for interacting with users and executing tools directly within Mattermost.\n\n![Version](https://img.shields.io/badge/version-0.1.0-blue)\n![Python](https://img.shields.io/badge/python-3.13.1%2B-blue)\n![License](https://img.shields.io/badge/license-MIT-green)\n![Package Manager](https://img.shields.io/badge/package%20manager-uv-purple)\n\n\n\n## Demo\n\n### 1. Github Agent in support channel - searches the existing issues and PRs and creates a new issue if not found\n![Description of your GIF](./demo/demo-3.gif)   \n\n\n### 2. Search internet and post to a channel using Mattermost-MCP-server\n![Description of your GIF](./demo/demo-2.gif)\n\n#### Scroll below for full demo in YouTube\n\n## Features\n\n- ü§ñ **Langgraph Agent Integration**: Uses a LangGraph agent to understand user requests and orchestrate responses.\n- üîå **MCP Server Integration**: Connects to multiple MCP servers defined in `mcp-servers.json`.\n- üõ†Ô∏è **Dynamic Tool Loading**: Automatically discovers tools from connected MCP servers and makes them available to the AI agent. Converts MCP tools to langchain structured tools.\n- üí¨ **Thread-Aware Conversations**: Maintains conversational context within Mattermost threads for coherent interactions.\n- üîÑ **Intelligent Tool Use**: The AI agent can decide when to use available tools (including chaining multiple calls) to fulfill user requests.\n- üîç **MCP Capability Discovery**: Allows users to list available servers, tools, resources, and prompts via direct commands.\n- #Ô∏è‚É£ **Direct Command Interface**: Interact directly with MCP servers using a command prefix (default: `#`).\n\n\n## Overview\n\nThe integration works as follows:\n\n1.  **Mattermost Connection (`mattermost_client.py`)**: Connects to the Mattermost server via API and WebSocket to listen for messages in a specified channel.\n2.  **MCP Connections (`mcp_client.py`)**: Establishes connections (primarily `stdio`) to each MCP server defined in `src/mattermost_mcp_host/mcp-servers.json`. It discovers available tools on each server.\n3.  **Agent Initialization (`agent/llm_agent.py`)**: A `LangGraphAgent` is created, configured with the chosen LLM provider and the dynamically loaded tools from all connected MCP servers.\n4.  **Message Handling (`main.py`)**:\n    *   If a message starts with the command prefix (`#`), it's parsed as a direct command to list servers/tools or call a specific tool via the corresponding `MCPClient`.\n    *   Otherwise, the message (along with thread history) is passed to the `LangGraphAgent`.\n5.  **Agent Execution**: The agent processes the request, potentially calling one or more MCP tools via the `MCPClient` instances, and generates a response.\n6.  **Response Delivery**: The final response from the agent or command execution is posted back to the appropriate Mattermost channel/thread.\n\n## Setup\n1.  **Clone the repository:**\n    ```bash\n    git clone <repository-url>\n    cd mattermost-mcp-host\n    ```\n\n2.  **Install:**\n    *   Using uv (recommended):\n        ```bash\n        # Install uv if you don't have it yet\n        # curl -LsSf https://astral.sh/uv/install.sh | sh \n\n        # Activate venv\n        source .venv/bin/activate\n        \n        # Install the package with uv\n        uv sync\n\n        # To install dev dependencies\n        uv sync --dev --all-extras\n        ```\n\n3.  **Configure Environment (`.env` file):**\n    Copy the `.env.example` and fill in the values or\n    Create a `.env` file in the project root (or set environment variables):\n    ```env\n    # Mattermost Details\n    MATTERMOST_URL=http://your-mattermost-url\n    MATTERMOST_TOKEN=your-bot-token # Needs permissions to post, read channel, etc.\n    MATTERMOST_TEAM_NAME=your-team-name\n    MATTERMOST_CHANNEL_NAME=your-channel-name # Channel for the bot to listen in\n    # MATTERMOST_CHANNEL_ID= # Optional: Auto-detected if name is provided\n\n    # LLM Configuration (Azure OpenAI is default)\n    DEFAULT_PROVIDER=azure\n    AZURE_OPENAI_ENDPOINT=your-azure-endpoint\n    AZURE_OPENAI_API_KEY=your-azure-api-key\n    AZURE_OPENAI_DEPLOYMENT=your-deployment-name # e.g., gpt-4o\n    # AZURE_OPENAI_API_VERSION= # Optional, defaults provided\n\n    # Optional: Other providers (install with `[all]` extra)\n    # OPENAI_API_KEY=...\n    # ANTHROPIC_API_KEY=...\n    # GOOGLE_API_KEY=...\n\n    # Command Prefix\n    COMMAND_PREFIX=# \n    ```\n    See `.env.example` for more options.\n\n4.  **Configure MCP Servers:**\n    Edit `src/mattermost_mcp_host/mcp-servers.json` to define the MCP servers you want to connect to. See `src/mattermost_mcp_host/mcp-servers-example.json`.\n    Depending on the server configuration, you might `npx`, `uvx`, `docker` installed in your system and in path.\n\n5.  **Start the Integration:**\n    ```bash\n    mattermost-mcp-host\n    ```\n\n\n## Prerequisites\n\n- Python 3.13.1+\n- uv package manager\n- Mattermost server instance\n- Mattermost Bot Account with API token\n- Access to a LLM API (Azure OpenAI)\n\n### Optional\n- One or more MCP servers configured in `mcp-servers.json` \n- Tavily web search requires `TAVILY_API_KEY` in `.env` file\n\n\n## Usage in Mattermost\n\nOnce the integration is running and connected:\n\n1.  **Direct Chat:** Simply chat in the configured channel or with the bot. The AI agent will respond, using tools as needed. It maintains context within message threads.\n2.  **Direct Commands:** Use the command prefix (default `#`) for specific actions:\n    *   `#help` - Display help information.\n    *   `#servers` - List configured and connected MCP servers.\n    *   `#<server_name> tools` - List available tools for `<server_name>`.\n    *   `#<server_name> call <tool_name> <json_arguments>` - Call `<tool_name>` on `<server_name>` with arguments provided as a JSON string.\n        *   Example: `#my-server call echo '{\"message\": \"Hello MCP!\"}'`\n    *   `#<server_name> resources` - List available resources for `<server_name>`.\n    *   `#<server_name> prompts` - List available prompts for `<server_name>`.\n\n\n\n## Next Steps\n- ‚öôÔ∏è **Configurable LLM Backend**: Supports multiple AI providers (Azure OpenAI default, OpenAI, Anthropic Claude, Google Gemini) via environment variables.\n\n## Mattermost Setup\n\n1. **Create a Bot Account**\n- Go to Integrations > Bot Accounts > Add Bot Account\n- Give it a name and description\n- Save the access token in the .env file\n\n2. **Required Bot Permissions**\n- post_all\n- create_post\n- read_channel\n- create_direct_channel\n- read_user\n\n3. **Add Bot to Team/Channel**\n- Invite the bot to your team\n- Add bot to desired channels\n\n### Troubleshooting\n\n1. **Connection Issues**\n- Verify Mattermost server is running\n- Check bot token permissions\n- Ensure correct team/channel names\n\n2. **AI Provider Issues**\n- Validate API keys\n- Check API quotas and limits\n- Verify network access to API endpoints\n\n3. **MCP Server Issues**\n- Check server logs\n- Verify server configurations\n- Ensure required dependencies are installed and env variables are defined\n\n\n## Demos\n\n### Create issue via chat using Github MCP server\n![Description of your GIF](./demo/demo-1.gif)  \n\n### (in YouTube)\n[![AI Agent in Action in Mattermost](./demo/supercut-thumbnail.png)](https://youtu.be/s6CZY81DRrU)\n\n\n## Contributing\n\nPlease feel free to open a PR.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n"
}