{
  "mcp_name": "xinlei413/DOC-Server-MCP",
  "mcp_description": "Fetches and searches third-party package documentation using versatile scraping and intelligent processing with hybrid search capabilities.",
  "mcp_id": "xinlei413_DOC-Server-MCP",
  "fetch_timestamp": "2025-06-23T09:32:48.902186Z",
  "github_url": "https://github.com/xinlei413/DOC-Server-MCP",
  "repository": {
    "name": "DOC-Server-MCP",
    "full_name": "xinlei413/DOC-Server-MCP",
    "description": null,
    "html_url": "https://github.com/xinlei413/DOC-Server-MCP",
    "created_at": "2025-04-28T05:45:41Z",
    "updated_at": "2025-04-28T05:46:14Z",
    "pushed_at": "2025-04-28T05:46:09Z",
    "size": 455,
    "stargazers_count": 0,
    "watchers_count": 0,
    "forks_count": 0,
    "open_issues_count": 0,
    "language": "TypeScript",
    "license": "MIT License",
    "topics": [],
    "default_branch": "main",
    "owner": {
      "login": "xinlei413",
      "type": "User",
      "avatar_url": "https://avatars.githubusercontent.com/u/97208211?v=4",
      "html_url": "https://github.com/xinlei413"
    },
    "has_issues": true,
    "has_projects": true,
    "has_downloads": true,
    "has_wiki": true,
    "has_pages": false,
    "archived": false,
    "disabled": false,
    "visibility": "public",
    "network_count": 0,
    "subscribers_count": 1,
    "languages": {
      "TypeScript": 507608,
      "JavaScript": 30579,
      "Dockerfile": 811
    },
    "language_percentages": {
      "TypeScript": 94.18,
      "JavaScript": 5.67,
      "Dockerfile": 0.15
    },
    "pull_requests_count": 0,
    "contributors_count": 1,
    "tags": [
      {
        "name": "MCP",
        "commit_sha": "9322529cbe95923d2e34dbfb7ec9dfa021c0fbc2"
      }
    ],
    "latest_version": "MCP",
    "package_json_version": "1.10.0"
  },
  "readme": "# docs-mcp-server MCP Server\n\nA MCP server for fetching and searching 3rd party package documentation.\n\n## ‚ú® Key Features\n\n- üåê **Versatile Scraping:** Fetch documentation from diverse sources like websites, GitHub, npm, PyPI, or local files.\n- üß† **Intelligent Processing:** Automatically split content semantically and generate embeddings using your choice of models (OpenAI, Google Gemini, Azure OpenAI, AWS Bedrock, Ollama, and more).\n- üíæ **Optimized Storage:** Leverage SQLite with `sqlite-vec` for efficient vector storage and FTS5 for robust full-text search.\n- üîç **Powerful Hybrid Search:** Combine vector similarity and full-text search across different library versions for highly relevant results.\n- ‚öôÔ∏è **Asynchronous Job Handling:** Manage scraping and indexing tasks efficiently with a background job queue and MCP/CLI tools.\n- üê≥ **Simple Deployment:** Get up and running quickly using Docker or npx.\n\n## Overview\n\nThis project provides a Model Context Protocol (MCP) server designed to scrape, process, index, and search documentation for various software libraries and packages. It fetches content from specified URLs, splits it into meaningful chunks using semantic splitting techniques, generates vector embeddings using OpenAI, and stores the data in an SQLite database. The server utilizes `sqlite-vec` for efficient vector similarity search and FTS5 for full-text search capabilities, combining them for hybrid search results. It supports versioning, allowing documentation for different library versions (including unversioned content) to be stored and queried distinctly.\n\nThe server exposes MCP tools for:\n\n- Starting a scraping job (`scrape_docs`): Returns a `jobId` immediately.\n- Checking job status (`get_job_status`): Retrieves the current status and progress of a specific job.\n- Listing active/completed jobs (`list_jobs`): Shows recent and ongoing jobs.\n- Cancelling a job (`cancel_job`): Attempts to stop a running or queued job.\n- Searching documentation (`search_docs`).\n- Listing indexed libraries (`list_libraries`).\n- Finding appropriate versions (`find_version`).\n- Removing indexed documents (`remove_docs`).\n- Fetching single URLs (`fetch_url`): Fetches a URL and returns its content as Markdown.\n\n## üÜï OpenRouter API ÈõÜÊàê‰∏éÂ§öÊ®°ÂûãÊîØÊåÅ\n\n### Chat/Completions ÂäüËÉΩ\n\nÊú¨ÊúçÂä°Â∑≤ÂÖ®Èù¢ÈÄÇÈÖç OpenRouter APIÔºåÊîØÊåÅ‰∏ªÊµÅÂ§ßÊ®°ÂûãÔºàGPT-4.1„ÄÅClaude 3.7„ÄÅGemini 2.5„ÄÅGrok„ÄÅQwen Á≠âÔºâÔºåÂπ∂ÊîØÊåÅÂ§öÊ®°ÊÄÅËæìÂÖ•ÔºàÊñáÊú¨+ÂõæÁâáÔºâ„ÄÇ\n\n#### ‰∏ªË¶ÅÁâπÊÄß\n- ‚úÖ ÊîØÊåÅ OpenRouter ÂÆòÊñπÊâÄÊúâ‰∏ªÊµÅÊ®°ÂûãÔºåÊ®°ÂûãÂàóË°®ËßÅ `src/utils/openrouter.ts` ÁöÑ `OPENROUTER_MODELS`\n- ‚úÖ ÊîØÊåÅÂ§öÊ®°ÊÄÅÊ∂àÊÅØÊ†ºÂºèÔºàÂ¶Ç text„ÄÅimage_urlÔºâ\n- ‚úÖ ÊîØÊåÅËá™ÂÆö‰πâ HTTP-Referer„ÄÅX-Title Á≠â headerÔºå‰æø‰∫é openrouter.ai ÁªüËÆ°ÂíåÊéíÂêç\n- ‚úÖ ÊîØÊåÅ OpenRouter API ÁöÑÊâÄÊúâÊâ©Â±ïÂèÇÊï∞ÔºàÂ¶Ç stream„ÄÅtools„ÄÅtemperature„ÄÅmax_tokens Á≠âÔºâ\n\n#### ÁéØÂ¢ÉÂèòÈáèÈÖçÁΩÆ\n- `OPENAI_API_KEY`ÔºöOpenRouter API KeyÔºàÂøÖÂ°´Ôºâ\n- `OPENAI_API_BASE`ÔºöOpenRouter API BaseÔºåÊé®Ëçê `https://openrouter.ai/api/v1`\n- `MODEL_ID`ÔºöÈªòËÆ§Ê®°ÂûãÔºàÂ¶Ç `openai/gpt-4.1`ÔºâÔºåÂèØÈÄâ\n\n#### Á§∫‰æã‰ª£Á†Å\n\n```typescript\nimport { openrouterChat } from './src/utils/openrouter';\n\nconst messages = [\n  {\n    role: 'user',\n    content: [\n      { type: 'text', text: 'What is in this image?' },\n      { type: 'image_url', image_url: { url: 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg' } }\n    ]\n  }\n];\n\nconst result = await openrouterChat({\n  model: 'openai/gpt-4.1',\n  messages,\n  referer: 'https://your-site.com', // ÂèØÈÄâ\n  xTitle: 'Your Site Name'           // ÂèØÈÄâ\n  // ËøòÂèØÂä† extraBody, headers Á≠âÂèÇÊï∞\n});\nconsole.log(result);\n```\n\n#### ÊîØÊåÅÁöÑ‰∏ªÊµÅÊ®°ÂûãÔºàÈÉ®ÂàÜÁ§∫‰æãÔºâ\n- openai/gpt-4.1\n- openai/gpt-4.1-mini\n- anthropic/claude-3.7-sonnet\n- google/gemini-2.5-pro-preview-03-25\n- x-ai/grok-3-beta\n- qwen/qwen2.5-vl-32b-instruct:free\n- deepseek/deepseek-chat-v3-0324:free\n- thudm/glm-z1-32b:free\n- openrouter/auto\n- ...ÔºàËØ¶ËßÅÊ∫êÁ†Å OPENROUTER_MODELSÔºâ\n\n#### Êõ¥Â§ö API ÂèÇÊï∞\nÂ¶ÇÈúÄÊîØÊåÅÊµÅÂºèËæìÂá∫„ÄÅÂáΩÊï∞Ë∞ÉÁî®„ÄÅsystem prompt„ÄÅstop„ÄÅtemperature„ÄÅmax_tokens Á≠â OpenRouter API ÂèÇÊï∞ÔºåÂè™ÈúÄÈÄöËøá `extraBody` Â≠óÊÆµ‰º†ÈÄíÂç≥ÂèØÔºåÊó†ÈúÄ‰øÆÊîπÂ∫ïÂ±Ç‰ª£Á†Å„ÄÇ\n\n## ‚ö†Ô∏è Embedding ÂäüËÉΩËØ¥Êòé\n\n> **Embedding ÂäüËÉΩÂ∑≤Á¶ÅÁî®ÔºÅ**\n>\n> Êú¨È°πÁõÆÂΩìÂâçÁâàÊú¨Â∑≤ÂΩªÂ∫ïÁßªÈô§ÊâÄÊúâ embedding Áõ∏ÂÖ≥ÂÆûÁé∞Âíå‰æùËµñÔºå‰∏çÂÜçÊîØÊåÅÂêëÈáèÁîüÊàê‰∏éÊ£ÄÁ¥¢„ÄÇÊâÄÊúâ embedding Áõ∏ÂÖ≥ API Âùá‰ºöÁõ¥Êé•ÊäõÂá∫ÂºÇÂ∏∏ÊèêÁ§∫„ÄÇ\n> \n> ‰ªÖ‰øùÁïôÂÖ®ÊñáÊ£ÄÁ¥¢‰∏éÂ§ßÊ®°Âûã chat/completions ËÉΩÂäõ„ÄÇ\n\n## Configuration\n\nThe following environment variables are supported to configure the embedding model behavior:\n\n### Embedding Model Configuration\n\n- `DOCS_MCP_EMBEDDING_MODEL`: **Optional.** Format: `provider:model_name` or just `model_name` (defaults to `text-embedding-3-small`). Supported providers and their required environment variables:\n\n  - `openai` (default): Uses OpenAI's embedding models\n\n    - `OPENAI_API_KEY`: **Required.** Your OpenAI API key\n    - `OPENAI_ORG_ID`: **Optional.** Your OpenAI Organization ID\n    - `OPENAI_API_BASE`: **Optional.** Custom base URL for OpenAI-compatible APIs (e.g., Ollama, Azure OpenAI)\n\n  - `vertex`: Uses Google Cloud Vertex AI embeddings\n\n    - `GOOGLE_APPLICATION_CREDENTIALS`: **Required.** Path to service account JSON key file\n\n  - `gemini`: Uses Google Generative AI (Gemini) embeddings\n\n    - `GOOGLE_API_KEY`: **Required.** Your Google API key\n\n  - `aws`: Uses AWS Bedrock embeddings\n\n    - `AWS_ACCESS_KEY_ID`: **Required.** AWS access key\n    - `AWS_SECRET_ACCESS_KEY`: **Required.** AWS secret key\n    - `AWS_REGION` or `BEDROCK_AWS_REGION`: **Required.** AWS region for Bedrock\n\n  - `microsoft`: Uses Azure OpenAI embeddings\n    - `AZURE_OPENAI_API_KEY`: **Required.** Azure OpenAI API key\n    - `AZURE_OPENAI_API_INSTANCE_NAME`: **Required.** Azure instance name\n    - `AZURE_OPENAI_API_DEPLOYMENT_NAME`: **Required.** Azure deployment name\n    - `AZURE_OPENAI_API_VERSION`: **Required.** Azure API version\n\n### Vector Dimensions\n\nThe database schema uses a fixed dimension of 1536 for embedding vectors. Only models that produce vectors with dimension ‚â§ 1536 are supported, except for certain providers (like Gemini) that support dimension reduction.\n\nFor OpenAI-compatible APIs (like Ollama), use the `openai` provider with `OPENAI_API_BASE` pointing to your endpoint.\n\nThese variables can be set regardless of how you run the server (Docker, npx, or from source).\n\n## Running the MCP Server\n\nThere are two ways to run the docs-mcp-server:\n\n### Option 1: Using Docker (Recommended)\n\nThis is the recommended approach for most users. It's easy, straightforward, and doesn't require Node.js to be installed.\n\n1. **Ensure Docker is installed and running.**\n2. **Configure your MCP settings:**\n\n   **Claude/Cline/Roo Configuration Example:**\n   Add the following configuration block to your MCP settings file (adjust path as needed):\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"docs-mcp-server\": {\n         \"command\": \"docker\",\n         \"args\": [\n           \"run\",\n           \"-i\",\n           \"--rm\",\n           \"-e\",\n           \"OPENAI_API_KEY\",\n           \"-v\",\n           \"docs-mcp-data:/data\",\n           \"ghcr.io/arabold/docs-mcp-server:latest\"\n         ],\n         \"env\": {\n           \"OPENAI_API_KEY\": \"sk-proj-...\" // Required: Replace with your key\n         },\n         \"disabled\": false,\n         \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n   Remember to replace `\"sk-proj-...\"` with your actual OpenAI API key and restart the application.\n\n3. **That's it!** The server will now be available to your AI assistant.\n\n**Docker Container Settings:**\n\n- `-i`: Keep STDIN open, crucial for MCP communication over stdio.\n- `--rm`: Automatically remove the container when it exits.\n- `-e OPENAI_API_KEY`: **Required.** Set your OpenAI API key.\n- `-v docs-mcp-data:/data`: **Required for persistence.** Mounts a Docker named volume `docs-mcp-data` to store the database. You can replace with a specific host path if preferred (e.g., `-v /path/on/host:/data`).\n\nAny of the configuration environment variables (see [Configuration](#configuration) above) can be passed to the container using the `-e` flag. For example:\n\n```bash\n# Example 1: Using OpenAI embeddings (default)\ndocker run -i --rm \\\n  -e OPENAI_API_KEY=\"your-key-here\" \\\n  -e DOCS_MCP_EMBEDDING_MODEL=\"text-embedding-3-small\" \\\n  -v docs-mcp-data:/data \\\n  ghcr.io/arabold/docs-mcp-server:latest\n\n# Example 2: Using OpenAI-compatible API (like Ollama)\ndocker run -i --rm \\\n  -e OPENAI_API_KEY=\"your-key-here\" \\\n  -e OPENAI_API_BASE=\"http://localhost:11434/v1\" \\\n  -e DOCS_MCP_EMBEDDING_MODEL=\"embeddings\" \\\n  -v docs-mcp-data:/data \\\n  ghcr.io/arabold/docs-mcp-server:latest\n\n# Example 3a: Using Google Cloud Vertex AI embeddings\ndocker run -i --rm \\\n  -e OPENAI_API_KEY=\"your-openai-key\" \\  # Keep for fallback to OpenAI\n  -e DOCS_MCP_EMBEDDING_MODEL=\"vertex:text-embedding-004\" \\\n  -e GOOGLE_APPLICATION_CREDENTIALS=\"/app/gcp-key.json\" \\\n  -v docs-mcp-data:/data \\\n  -v /path/to/gcp-key.json:/app/gcp-key.json:ro \\\n  ghcr.io/arabold/docs-mcp-server:latest\n\n# Example 3b: Using Google Generative AI (Gemini) embeddings\ndocker run -i --rm \\\n  -e OPENAI_API_KEY=\"your-openai-key\" \\  # Keep for fallback to OpenAI\n  -e DOCS_MCP_EMBEDDING_MODEL=\"gemini:embedding-001\" \\\n  -e GOOGLE_API_KEY=\"your-google-api-key\" \\\n  -v docs-mcp-data:/data \\\n  ghcr.io/arabold/docs-mcp-server:latest\n\n# Example 4: Using AWS Bedrock embeddings\ndocker run -i --rm \\\n  -e AWS_ACCESS_KEY_ID=\"your-aws-key\" \\\n  -e AWS_SECRET_ACCESS_KEY=\"your-aws-secret\" \\\n  -e AWS_REGION=\"us-east-1\" \\\n  -e DOCS_MCP_EMBEDDING_MODEL=\"aws:amazon.titan-embed-text-v1\" \\\n  -v docs-mcp-data:/data \\\n  ghcr.io/arabold/docs-mcp-server:latest\n\n# Example 5: Using Azure OpenAI embeddings\ndocker run -i --rm \\\n  -e AZURE_OPENAI_API_KEY=\"your-azure-key\" \\\n  -e AZURE_OPENAI_API_INSTANCE_NAME=\"your-instance\" \\\n  -e AZURE_OPENAI_API_DEPLOYMENT_NAME=\"your-deployment\" \\\n  -e AZURE_OPENAI_API_VERSION=\"2024-02-01\" \\\n  -e DOCS_MCP_EMBEDDING_MODEL=\"microsoft:text-embedding-ada-002\" \\\n  -v docs-mcp-data:/data \\\n  ghcr.io/arabold/docs-mcp-server:latest\n```\n\n### Option 2: Using npx\n\nThis approach is recommended when you need local file access (e.g., indexing documentation from your local file system). While this can also be achieved by mounting paths into a Docker container, using npx is simpler but requires a Node.js installation.\n\n1. **Ensure Node.js is installed.**\n2. **Configure your MCP settings:**\n\n   **Claude/Cline/Roo Configuration Example:**\n   Add the following configuration block to your MCP settings file:\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"docs-mcp-server\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"--package=@arabold/docs-mcp-server\", \"docs-server\"],\n         \"env\": {\n           \"OPENAI_API_KEY\": \"sk-proj-...\" // Required: Replace with your key\n         },\n         \"disabled\": false,\n         \"autoApprove\": []\n       }\n     }\n   }\n   ```\n\n   Remember to replace `\"sk-proj-...\"` with your actual OpenAI API key and restart the application.\n\n3. **That's it!** The server will now be available to your AI assistant.\n\n## Using the CLI\n\nYou can use the CLI to manage documentation directly, either via Docker or npx. **Important: Use the same method (Docker or npx) for both the server and CLI to ensure access to the same indexed documentation.**\n\n### Using Docker CLI\n\nIf you're running the server with Docker, use Docker for the CLI as well:\n\n```bash\ndocker run --rm \\\n  -e OPENAI_API_KEY=\"your-openai-api-key-here\" \\\n  -v docs-mcp-data:/data \\\n  ghcr.io/arabold/docs-mcp-server:latest \\\n  docs-cli <command> [options]\n```\n\nMake sure to use the same volume name (`docs-mcp-data` in this example) as you did for the server. Any of the configuration environment variables (see [Configuration](#configuration) above) can be passed using `-e` flags, just like with the server.\n\n### Using npx CLI\n\nIf you're running the server with npx, use npx for the CLI as well:\n\n```bash\nnpx -y --package=@arabold/docs-mcp-server docs-cli <command> [options]\n```\n\nThe npx approach will use the default data directory on your system (typically in your home directory), ensuring consistency between server and CLI.\n\n(See \"CLI Command Reference\" below for available commands and options.)\n\n### CLI Command Reference\n\nThe `docs-cli` provides commands for managing the documentation index. Access it either via Docker (`docker run -v docs-mcp-data:/data ghcr.io/arabold/docs-mcp-server:latest docs-cli ...`) or `npx` (`npx -y --package=@arabold/docs-mcp-server docs-cli ...`).\n\n**General Help:**\n\n```bash\ndocs-cli --help\n# or\nnpx -y --package=@arabold/docs-mcp-server docs-cli --help\n```\n\n**Command Specific Help:** (Replace `docs-cli` with the `npx...` command if not installed globally)\n\n```bash\ndocs-cli scrape --help\ndocs-cli search --help\ndocs-cli fetch-url --help\ndocs-cli find-version --help\ndocs-cli remove --help\ndocs-cli list --help\n```\n\n### Fetching Single URLs (`fetch-url`)\n\nFetches a single URL and converts its content to Markdown. Unlike `scrape`, this command does not crawl links or store the content.\n\n```bash\ndocs-cli fetch-url <url> [options]\n```\n\n**Options:**\n\n- `--no-follow-redirects`: Disable following HTTP redirects (default: follow redirects).\n- `--scrape-mode <mode>`: HTML processing strategy: 'fetch' (fast, less JS), 'playwright' (slow, full JS), 'auto' (default).\n\n**Examples:**\n\n```bash\n# Fetch a URL and convert to Markdown\ndocs-cli fetch-url https://example.com/page.html\n```\n\n### Scraping Documentation (`scrape`)\n\nScrapes and indexes documentation from a given URL for a specific library.\n\n```bash\ndocs-cli scrape <library> <url> [options]\n```\n\n**Options:**\n\n- `-v, --version <string>`: The specific version to associate with the scraped documents.\n  - Accepts full versions (`1.2.3`), pre-release versions (`1.2.3-beta.1`), or partial versions (`1`, `1.2` which are expanded to `1.0.0`, `1.2.0`).\n  - If omitted, the documentation is indexed as **unversioned**.\n- `-p, --max-pages <number>`: Maximum pages to scrape (default: 1000).\n- `-d, --max-depth <number>`: Maximum navigation depth (default: 3).\n- `-c, --max-concurrency <number>`: Maximum concurrent requests (default: 3).\n- `--scope <scope>`: Defines the crawling boundary: 'subpages' (default), 'hostname', or 'domain'.\n- `--no-follow-redirects`: Disable following HTTP redirects (default: follow redirects).\n- `--scrape-mode <mode>`: HTML processing strategy: 'fetch' (fast, less JS), 'playwright' (slow, full JS), 'auto' (default).\n- `--ignore-errors`: Ignore errors during scraping (default: true).\n\n**Examples:**\n\n```bash\n# Scrape React 18.2.0 docs\ndocs-cli scrape react --version 18.2.0 https://react.dev/\n```\n\n### Searching Documentation (`search`)\n\nSearches the indexed documentation for a library, optionally filtering by version.\n\n```bash\ndocs-cli search <library> <query> [options]\n```\n\n**Options:**\n\n- `-v, --version <string>`: The target version or range to search within.\n  - Supports exact versions (`18.0.0`), partial versions (`18`), or ranges (`18.x`).\n  - If omitted, searches the **latest** available indexed version.\n  - If a specific version/range doesn't match, it falls back to the latest indexed version _older_ than the target.\n  - To search **only unversioned** documents, explicitly pass an empty string: `--version \"\"`. (Note: Omitting `--version` searches latest, which _might_ be unversioned if no other versions exist).\n- `-l, --limit <number>`: Maximum number of results (default: 5).\n- `-e, --exact-match`: Only match the exact version specified (disables fallback and range matching) (default: false).\n\n**Examples:**\n\n```bash\n# Search latest React docs for 'hooks'\ndocs-cli search react 'hooks'\n```\n\n### Finding Available Versions (`find-version`)\n\nChecks the index for the best matching version for a library based on a target, and indicates if unversioned documents exist.\n\n```bash\ndocs-cli find-version <library> [options]\n```\n\n**Options:**\n\n- `-v, --version <string>`: The target version or range. If omitted, finds the latest available version.\n\n**Examples:**\n\n```bash\n# Find the latest indexed version for react\ndocs-cli find-version react\n```\n\n### Listing Libraries (`list`)\n\nLists all libraries currently indexed in the store.\n\n```bash\ndocs-cli list\n```\n\n### Removing Documentation (`remove`)\n\nRemoves indexed documents for a specific library and version.\n\n```bash\ndocs-cli remove <library> [options]\n```\n\n**Options:**\n\n- `-v, --version <string>`: The specific version to remove. If omitted, removes **unversioned** documents for the library.\n\n**Examples:**\n\n```bash\n# Remove React 18.2.0 docs\ndocs-cli remove react --version 18.2.0\n```\n\n### Version Handling Summary\n\n- **Scraping:** Requires a specific, valid version (`X.Y.Z`, `X.Y.Z-pre`, `X.Y`, `X`) or no version (for unversioned docs). Ranges (`X.x`) are invalid for scraping.\n- **Searching/Finding:** Accepts specific versions, partials, or ranges (`X.Y.Z`, `X.Y`, `X`, `X.x`). Falls back to the latest older version if the target doesn't match. Omitting the version targets the latest available. Explicitly searching `--version \"\"` targets unversioned documents.\n- **Unversioned Docs:** Libraries can have documentation stored without a specific version (by omitting `--version` during scrape). These can be searched explicitly using `--version \"\"`. The `find-version` command will also report if unversioned docs exist alongside any semver matches.\n\n## Êú¨Âú∞ÈÖçÁΩÆ OpenRouter ËØ¶ÁªÜÊ≠•È™§ÔºàÈõ∂Âü∫Á°ÄÊìç‰ΩúÊåáÂºïÔºâ\n\n### 1. ÊâìÂºÄ .env Êñá‰ª∂\n- Ë∑ØÂæÑÔºö`~/MCP/MCP DOC Server/docs-mcp-server-main/.env`\n- Áî®ÊñáÊú¨ÁºñËæëÂô®ÔºàÂ¶Ç TextEdit„ÄÅËÆ∞‰∫ãÊú¨„ÄÅVSCodeÔºâÊâìÂºÄ„ÄÇ\n\n### 2. Â°´ÂÜô‰Ω†ÁöÑ OpenRouter ‰ø°ÊÅØ\nÁî®‰∏ãÈù¢ÂÜÖÂÆπÊõøÊç¢ÔºàÊàñË°•ÂÖÖÔºâ‰Ω†ÁöÑ `.env` Êñá‰ª∂Ôºö\n\n```\nOPENAI_API_KEY=‰Ω†ÁöÑOpenRouter Key\nOPENAI_API_BASE=https://openrouter.ai/api/v1\nMODEL_ID=qwen/qwen2.5-vl-3b-instruct:free\n```\n\n> ËØ¥ÊòéÔºö\n> - `OPENAI_API_KEY` Áî®‰Ω†ÁöÑ OpenRouter KeyÔºàÂ¶Ç‰∏äÁ§∫‰æãÔºâ„ÄÇ\n> - `OPENAI_API_BASE` Âõ∫ÂÆö‰∏∫ `https://openrouter.ai/api/v1`„ÄÇ\n> - `MODEL_ID` Â°´ `qwen/qwen2.5-vl-3b-instruct:free`„ÄÇ\n\n### 3. ‰øùÂ≠ò .env Êñá‰ª∂\n\n### 4. ÈáçÂêØ MCP Server\n- ÂÖ≥Èó≠‰πãÂâçÁöÑ MCP Server ÁªàÁ´ØÁ™óÂè£ÔºàÂ¶ÇÊúâÔºâ„ÄÇ\n- Âú® MCP ÁõÆÂΩï‰∏ãÈáçÊñ∞ËøêË°åÔºö\n\n```bash\nnpm run dev:server\n```\n\n- Á≠âÂæÖÂá∫Áé∞ `Build success`„ÄÅ`Watching for changes` Â≠óÊ†∑„ÄÇ\n\n---\n\nÂ¶ÇÈÅáÂà∞‰ªª‰ΩïÈóÆÈ¢òÔºåÊääÊä•ÈîôÂÜÖÂÆπÂèëÁªôÂºÄÂèëËÄÖÊàñÊäÄÊúØÊîØÊåÅÂç≥ÂèØ„ÄÇ\n\n## Development & Advanced Setup\n\nThis section covers running the server/CLI directly from the source code for development purposes. The primary usage method is now via the public Docker image as described in \"Method 2\".\n\n### Running from Source (Development)\n\nThis provides an isolated environment and exposes the server via HTTP endpoints.\n\n1.  **Clone the repository:**\n    ```bash\n    git clone https://github.com/arabold/docs-mcp-server.git # Replace with actual URL if different\n    cd docs-mcp-server\n    ```\n2.  **Create `.env` file:**\n    Copy the example and add your OpenAI key (see \"Environment Setup\" below).\n    ```bash\n    cp .env.example .env\n    # Edit .env and add your OPENAI_API_KEY\n    ```\n3.  **Build the Docker image:**\n    ```bash\n    docker build -t docs-mcp-server .\n    ```\n4.  **Run the Docker container:**\n\n    ```bash\n    # Option 1: Using a named volume (recommended)\n    # Docker automatically creates the volume 'docs-mcp-data' if it doesn't exist on first run.\n    docker run -i --env-file .env -v docs-mcp-data:/data --name docs-mcp-server docs-mcp-server\n\n    # Option 2: Mapping to a host directory\n    # docker run -i --env-file .env -v /path/on/your/host:/data --name docs-mcp-server docs-mcp-server\n    ```\n\n    - `-i`: Keep STDIN open even if not attached. This is crucial for interacting with the server over stdio.\n    - `--env-file .env`: Loads environment variables (like `OPENAI_API_KEY`) from your local `.env` file.\n    - `-v docs-mcp-data:/data` or `-v /path/on/your/host:/data`: **Crucial for persistence.** This mounts a Docker named volume (Docker creates `docs-mcp-data` automatically if needed) or a host directory to the `/data` directory inside the container. The `/data` directory is where the server stores its `documents.db` file (as configured by `DOCS_MCP_STORE_PATH` in the Dockerfile). This ensures your indexed documentation persists even if the container is stopped or removed.\n    - `--name docs-mcp-server`: Assigns a convenient name to the container.\n\n    The server inside the container now runs directly using Node.js and communicates over **stdio**.\n\nThis method is useful for contributing to the project or running un-published versions.\n\n1.  **Clone the repository:**\n    ```bash\n    git clone https://github.com/arabold/docs-mcp-server.git # Replace with actual URL if different\n    cd docs-mcp-server\n    ```\n2.  **Install dependencies:**\n    ```bash\n    npm install\n    ```\n3.  **Build the project:**\n    This compiles TypeScript to JavaScript in the `dist/` directory.\n    ```bash\n    npm run build\n    ```\n4.  **Setup Environment:**\n    Create and configure your `.env` file as described in \"Environment Setup\" below. This is crucial for providing the `OPENAI_API_KEY`.\n\n5.  **Run:**\n    - **Server (Development Mode):** `npm run dev:server` (builds, watches, and restarts)\n    - **Server (Production Mode):** `npm run start` (runs pre-built code)\n    - **CLI:** `npm run cli -- <command> [options]` or `node dist/cli.js <command> [options]`\n\n### Environment Setup (for Source/Docker)\n\n**Note:** This `.env` file setup is primarily needed when running the server from source or using the Docker method. When using the `npx` integration method, the `OPENAI_API_KEY` is set directly in the MCP configuration file.\n\n1. Create a `.env` file based on `.env.example`:\n   ```bash\n   cp .env.example .env\n   ```\n2. Update your OpenAI API key in `.env`:\n\n   ```\n   # Required: Your OpenAI API key for generating embeddings.\n   OPENAI_API_KEY=your-api-key-here\n\n   # Optional: Your OpenAI Organization ID (handled automatically by LangChain if set)\n   OPENAI_ORG_ID=\n\n   # Optional: Custom base URL for OpenAI API (e.g., for Azure OpenAI or compatible APIs)\n   OPENAI_API_BASE=\n\n   # Optional: Embedding model name (defaults to \"text-embedding-3-small\")\n   # Examples: text-embedding-3-large, text-embedding-ada-002\n   DOCS_MCP_EMBEDDING_MODEL=\n\n   # Optional: Specify a custom directory to store the SQLite database file (documents.db).\n   # If set, this path takes precedence over the default locations.\n   # Default behavior (if unset):\n   # 1. Uses './.store/' in the project root if it exists (legacy).\n   # 2. Falls back to OS-specific data directory (e.g., ~/Library/Application Support/docs-mcp-server on macOS).\n   # DOCS_MCP_STORE_PATH=/path/to/your/desired/storage/directory\n   ```\n\n### Debugging (from Source)\n\nSince MCP servers communicate over stdio when run directly via Node.js, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script after building:\n\n```bash\nnpx @modelcontextprotocol/inspector node dist/server.js\n```\n\nThe Inspector will provide a URL to access debugging tools in your browser.\n\n### Releasing\n\nThis project uses [semantic-release](https://github.com/semantic-release/semantic-release) and [Conventional Commits](https://www.conventionalcommits.org/) to automate the release process.\n\n**How it works:**\n\n1.  **Commit Messages:** All commits merged into the `main` branch **must** follow the Conventional Commits specification.\n2.  **Manual Trigger:** The \"Release\" GitHub Actions workflow can be triggered manually from the Actions tab when you're ready to create a new release.\n3.  **`semantic-release` Actions:** Determines version, updates `CHANGELOG.md` & `package.json`, commits, tags, publishes to npm, and creates a GitHub Release.\n\n**What you need to do:**\n\n- Use Conventional Commits.\n- Merge changes to `main`.\n- Trigger a release manually when ready from the Actions tab in GitHub.\n\n**Automation handles:** Changelog, version bumps, tags, npm publish, GitHub releases.\n\n### Architecture\n\nFor details on the project's architecture and design principles, please see [ARCHITECTURE.md](ARCHITECTURE.md).\n\n_Notably, the vast majority of this project's code was generated by the AI assistant Cline, leveraging the capabilities of this very MCP server._\n"
}