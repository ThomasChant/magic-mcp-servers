{
  "mcp_name": "pydantic/pydantic-ai/mcp-run-python",
  "mcp_description": "üêç üè†- Run Python code in a secure sandbox via MCP tool calls",
  "mcp_id": "pydantic_pydantic-ai",
  "fetch_timestamp": "2025-06-23T07:40:12.543494Z",
  "github_url": "https://github.com/pydantic/pydantic-ai/tree/main/mcp-run-python",
  "repository": {
    "name": "pydantic-ai",
    "full_name": "pydantic/pydantic-ai",
    "description": "Agent Framework / shim to use Pydantic with LLMs",
    "html_url": "https://github.com/pydantic/pydantic-ai",
    "created_at": "2024-06-21T15:55:04Z",
    "updated_at": "2025-06-23T07:17:31Z",
    "pushed_at": "2025-06-22T22:12:49Z",
    "size": 27018,
    "stargazers_count": 10381,
    "watchers_count": 10381,
    "forks_count": 961,
    "open_issues_count": 321,
    "language": "Python",
    "license": "MIT License",
    "topics": [
      "agent-framework",
      "llms",
      "pydantic",
      "python"
    ],
    "default_branch": "main",
    "owner": {
      "login": "pydantic",
      "type": "Organization",
      "avatar_url": "https://avatars.githubusercontent.com/u/110818415?v=4",
      "html_url": "https://github.com/pydantic"
    },
    "has_issues": true,
    "has_projects": true,
    "has_downloads": true,
    "has_wiki": false,
    "has_pages": false,
    "archived": false,
    "disabled": false,
    "visibility": "public",
    "network_count": 961,
    "subscribers_count": 75,
    "languages": {
      "Python": 2401925,
      "TypeScript": 14988,
      "Makefile": 6135
    },
    "language_percentages": {
      "Python": 99.13,
      "TypeScript": 0.62,
      "Makefile": 0.25
    },
    "pull_requests_count": 954,
    "contributors_count": 149,
    "latest_release": {
      "tag_name": "v0.3.2",
      "name": "v0.3.2 (2025-06-21)",
      "published_at": "2025-06-21T05:16:27Z",
      "body": "## What's Changed\r\n* Support MCP sampling by @samuelcolvin in https://github.com/pydantic/pydantic-ai/pull/1884\r\n\r\n\r\n**Full Changelog**: https://github.com/pydantic/pydantic-ai/compare/v0.3.1...v0.3.2",
      "prerelease": false,
      "draft": false
    },
    "tags": [
      {
        "name": "v0.3.2",
        "commit_sha": "a25eb963a54e07afeab5ca2ea143437225100638"
      },
      {
        "name": "v0.3.1",
        "commit_sha": "f2646dedc34fcdd408b665753df1967e22e3f7a6"
      },
      {
        "name": "v0.3.0",
        "commit_sha": "b487d60e16b41bd207dc09c2fc58ff2e8a164e61"
      },
      {
        "name": "v0.2.20",
        "commit_sha": "7fdd74501fafdf37d38bcea43544d9521f3205c2"
      },
      {
        "name": "v0.2.19",
        "commit_sha": "a953d3480f9d58bd1d3c74ab099871f5e77e83b1"
      },
      {
        "name": "v0.2.18",
        "commit_sha": "67c381ecd3d389b55c3f3a3396591b930f9c4f80"
      },
      {
        "name": "v0.2.17",
        "commit_sha": "2fce134e3b0c1b7d762bc922623fd8f84c450bcd"
      },
      {
        "name": "v0.2.16",
        "commit_sha": "78e006c8c7b1e087d57c50dacb00587b46fbdac5"
      },
      {
        "name": "v0.2.15",
        "commit_sha": "ea837b9ddbe7809e1d8288447f7f0fe8e5a936a8"
      },
      {
        "name": "v0.2.14",
        "commit_sha": "aeaa8cfe7586e6e9e00e27ab0f8abebcf68f9ce8"
      }
    ],
    "latest_version": "v0.3.2"
  },
  "readme": "<div align=\"center\">\n  <a href=\"https://ai.pydantic.dev/\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://ai.pydantic.dev/img/pydantic-ai-dark.svg\">\n      <img src=\"https://ai.pydantic.dev/img/pydantic-ai-light.svg\" alt=\"PydanticAI\">\n    </picture>\n  </a>\n</div>\n<div align=\"center\">\n  <em>Agent Framework / shim to use Pydantic with LLMs</em>\n</div>\n<div align=\"center\">\n  <a href=\"https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml?query=branch%3Amain\"><img src=\"https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml/badge.svg?event=push\" alt=\"CI\"></a>\n  <a href=\"https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic-ai\"><img src=\"https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic-ai.svg\" alt=\"Coverage\"></a>\n  <a href=\"https://pypi.python.org/pypi/pydantic-ai\"><img src=\"https://img.shields.io/pypi/v/pydantic-ai.svg\" alt=\"PyPI\"></a>\n  <a href=\"https://github.com/pydantic/pydantic-ai\"><img src=\"https://img.shields.io/pypi/pyversions/pydantic-ai.svg\" alt=\"versions\"></a>\n  <a href=\"https://github.com/pydantic/pydantic-ai/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/pydantic/pydantic-ai.svg?v\" alt=\"license\"></a>\n  <a href=\"https://logfire.pydantic.dev/docs/join-slack/\"><img src=\"https://img.shields.io/badge/Slack-Join%20Slack-4A154B?logo=slack\" alt=\"Join Slack\" /></a>\n</div>\n\n---\n\n**Documentation**: [ai.pydantic.dev](https://ai.pydantic.dev/)\n\n---\n\nPydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI.\n\nFastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation of [Pydantic](https://docs.pydantic.dev).\n\nSimilarly, virtually every agent framework and LLM library in Python uses Pydantic, yet when we began to use LLMs in [Pydantic Logfire](https://pydantic.dev/logfire), we couldn't find anything that gave us the same feeling.\n\nWe built PydanticAI with one simple aim: to bring that FastAPI feeling to GenAI app development.\n\n## Why use PydanticAI\n\n* __Built by the Pydantic Team__\nBuilt by the team behind [Pydantic](https://docs.pydantic.dev/latest/) (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more).\n\n* __Model-agnostic__\nSupports OpenAI, Anthropic, Gemini, Deepseek, Ollama, Groq, Cohere, and Mistral, and there is a simple interface to implement support for [other models](https://ai.pydantic.dev/models/).\n\n* __Pydantic Logfire Integration__\nSeamlessly [integrates](https://ai.pydantic.dev/logfire/) with [Pydantic Logfire](https://pydantic.dev/logfire) for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications.\n\n* __Type-safe__\nDesigned to make [type checking](https://ai.pydantic.dev/agents/#static-type-checking) as powerful and informative as possible for you.\n\n* __Python-centric Design__\nLeverages Python's familiar control flow and agent composition to build your AI-driven projects, making it easy to apply standard Python best practices you'd use in any other (non-AI) project.\n\n* __Structured Responses__\nHarnesses the power of [Pydantic](https://docs.pydantic.dev/latest/) to [validate and structure](https://ai.pydantic.dev/output/#structured-output) model outputs, ensuring responses are consistent across runs.\n\n* __Dependency Injection System__\nOffers an optional [dependency injection](https://ai.pydantic.dev/dependencies/) system to provide data and services to your agent's [system prompts](https://ai.pydantic.dev/agents/#system-prompts), [tools](https://ai.pydantic.dev/tools/) and [output validators](https://ai.pydantic.dev/output/#output-validator-functions).\nThis is useful for testing and eval-driven iterative development.\n\n* __Streamed Responses__\nProvides the ability to [stream](https://ai.pydantic.dev/output/#streamed-results) LLM outputs continuously, with immediate validation, ensuring rapid and accurate outputs.\n\n* __Graph Support__\n[Pydantic Graph](https://ai.pydantic.dev/graph) provides a powerful way to define graphs using typing hints, this is useful in complex applications where standard control flow can degrade to spaghetti code.\n\n## Hello World Example\n\nHere's a minimal example of PydanticAI:\n\n```python\nfrom pydantic_ai import Agent\n\n# Define a very simple agent including the model to use, you can also set the model when running the agent.\nagent = Agent(\n    'google-gla:gemini-1.5-flash',\n    # Register a static system prompt using a keyword argument to the agent.\n    # For more complex dynamically-generated system prompts, see the example below.\n    system_prompt='Be concise, reply with one sentence.',\n)\n\n# Run the agent synchronously, conducting a conversation with the LLM.\n# Here the exchange should be very short: PydanticAI will send the system prompt and the user query to the LLM,\n# the model will return a text response. See below for a more complex run.\nresult = agent.run_sync('Where does \"hello world\" come from?')\nprint(result.output)\n\"\"\"\nThe first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\n\"\"\"\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nNot very interesting yet, but we can easily add \"tools\", dynamic system prompts, and structured responses to build more powerful agents.\n\n## Tools & Dependency Injection Example\n\nHere is a concise example using PydanticAI to build a support agent for a bank:\n\n**(Better documented example [in the docs](https://ai.pydantic.dev/#tools-dependency-injection-example))**\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent, RunContext\n\nfrom bank_database import DatabaseConn\n\n\n# SupportDependencies is used to pass data, connections, and logic into the model that will be needed when running\n# system prompt and tool functions. Dependency injection provides a type-safe way to customise the behavior of your agents.\n@dataclass\nclass SupportDependencies:\n    customer_id: int\n    db: DatabaseConn\n\n\n# This pydantic model defines the structure of the output returned by the agent.\nclass SupportOutput(BaseModel):\n    support_advice: str = Field(description='Advice returned to the customer')\n    block_card: bool = Field(description=\"Whether to block the customer's card\")\n    risk: int = Field(description='Risk level of query', ge=0, le=10)\n\n\n# This agent will act as first-tier support in a bank.\n# Agents are generic in the type of dependencies they accept and the type of output they return.\n# In this case, the support agent has type `Agent[SupportDependencies, SupportOutput]`.\nsupport_agent = Agent(\n    'openai:gpt-4o',\n    deps_type=SupportDependencies,\n    # The response from the agent will, be guaranteed to be a SupportOutput,\n    # if validation fails the agent is prompted to try again.\n    output_type=SupportOutput,\n    system_prompt=(\n        'You are a support agent in our bank, give the '\n        'customer support and judge the risk level of their query.'\n    ),\n)\n\n\n# Dynamic system prompts can make use of dependency injection.\n# Dependencies are carried via the `RunContext` argument, which is parameterized with the `deps_type` from above.\n# If the type annotation here is wrong, static type checkers will catch it.\n@support_agent.system_prompt\nasync def add_customer_name(ctx: RunContext[SupportDependencies]) -> str:\n    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\n    return f\"The customer's name is {customer_name!r}\"\n\n\n# `tool` let you register functions which the LLM may call while responding to a user.\n# Again, dependencies are carried via `RunContext`, any other arguments become the tool schema passed to the LLM.\n# Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry.\n@support_agent.tool\nasync def customer_balance(\n        ctx: RunContext[SupportDependencies], include_pending: bool\n) -> float:\n    \"\"\"Returns the customer's current account balance.\"\"\"\n    # The docstring of a tool is also passed to the LLM as the description of the tool.\n    # Parameter descriptions are extracted from the docstring and added to the parameter schema sent to the LLM.\n    balance = await ctx.deps.db.customer_balance(\n        id=ctx.deps.customer_id,\n        include_pending=include_pending,\n    )\n    return balance\n\n\n...  # In a real use case, you'd add more tools and a longer system prompt\n\n\nasync def main():\n    deps = SupportDependencies(customer_id=123, db=DatabaseConn())\n    # Run the agent asynchronously, conducting a conversation with the LLM until a final response is reached.\n    # Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve an output.\n    result = await support_agent.run('What is my balance?', deps=deps)\n    # The `result.output` will be validated with Pydantic to guarantee it is a `SupportOutput`. Since the agent is generic,\n    # it'll also be typed as a `SupportOutput` to aid with static type checking.\n    print(result.output)\n    \"\"\"\n    support_advice='Hello John, your current account balance, including pending transactions, is $123.45.' block_card=False risk=1\n    \"\"\"\n\n    result = await support_agent.run('I just lost my card!', deps=deps)\n    print(result.output)\n    \"\"\"\n    support_advice=\"I'm sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.\" block_card=True risk=8\n    \"\"\"\n```\n\n## Next Steps\n\nTo try PydanticAI yourself, follow the instructions [in the examples](https://ai.pydantic.dev/examples/).\n\nRead the [docs](https://ai.pydantic.dev/agents/) to learn more about building applications with PydanticAI.\n\nRead the [API Reference](https://ai.pydantic.dev/api/agent/) to understand PydanticAI's interface.\n"
}