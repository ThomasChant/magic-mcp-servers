{
  "mcp_name": "chrishayuk/mcp-cli",
  "mcp_description": "üêç üè† - Yet another CLI tool for testing MCP servers",
  "mcp_id": "chrishayuk_mcp-cli",
  "fetch_timestamp": "2025-06-23T02:05:11.657349Z",
  "github_url": "https://github.com/chrishayuk/mcp-cli",
  "repository": {
    "name": "mcp-cli",
    "full_name": "chrishayuk/mcp-cli",
    "description": null,
    "html_url": "https://github.com/chrishayuk/mcp-cli",
    "created_at": "2024-11-30T00:06:19Z",
    "updated_at": "2025-06-22T22:00:29Z",
    "pushed_at": "2025-06-22T13:26:45Z",
    "size": 1015,
    "stargazers_count": 1312,
    "watchers_count": 1312,
    "forks_count": 233,
    "open_issues_count": 18,
    "language": "Python",
    "license": "Other",
    "topics": [],
    "default_branch": "main",
    "owner": {
      "login": "chrishayuk",
      "type": "User",
      "avatar_url": "https://avatars.githubusercontent.com/u/690076?v=4",
      "html_url": "https://github.com/chrishayuk"
    },
    "has_issues": true,
    "has_projects": true,
    "has_downloads": true,
    "has_wiki": true,
    "has_pages": false,
    "archived": false,
    "disabled": false,
    "visibility": "public",
    "network_count": 233,
    "subscribers_count": 24,
    "languages": {
      "Python": 527463,
      "Makefile": 2758
    },
    "language_percentages": {
      "Python": 99.48,
      "Makefile": 0.52
    },
    "pull_requests_count": 82,
    "contributors_count": 13
  },
  "readme": "# MCP CLI - Model Context Protocol Command Line Interface\n\nA powerful, feature-rich command-line interface for interacting with Model Context Protocol servers. This client enables seamless communication with LLMs through integration with the [CHUK-MCP protocol library](https://github.com/chrishayuk/chuk-mcp) which is a pyodide compatible pure python protocol implementation of MCP, supporting tool usage, conversation management, and multiple operational modes.\n\n## üîÑ Protocol Implementation\n\nThe core protocol implementation has been moved to a separate package at:\n**[https://github.com/chrishayuk/chuk-mcp](https://github.com/chrishayuk/chuk-mcp)**\n\nThis CLI is built on top of the protocol library, focusing on providing a rich user experience while the protocol library handles the communication layer.\n\n## üåü Features\n\n- **Multiple Operational Modes**:\n  - **Chat Mode**: Conversational interface with direct LLM interaction and automated tool usage\n  - **Interactive Mode**: Command-driven interface for direct server operations\n  - **Command Mode**: Unix-friendly mode for scriptable automation and pipelines\n  - **Direct Commands**: Run individual commands without entering interactive mode\n\n- **Multi-Provider Support**:\n  - OpenAI integration (`gpt-4o-mini`, `gpt-4o`, `gpt-4-turbo`, etc.)\n  - Ollama integration (`llama3.2`, `qwen2.5-coder`, etc.)\n  - Anthropic integration (`claude-3-opus`, `claude-3-sonnet`, etc.)\n  - Extensible architecture for additional providers\n\n- **Provider and Model Management**:\n  - Configure multiple LLM providers (API keys, endpoints, default models)\n  - Switch between providers and models during sessions\n  - Command-line arguments for provider/model selection\n  - Interactive commands for provider configuration\n\n- **Robust Tool System**:\n  - Automatic discovery of server-provided tools\n  - Server-aware tool execution\n  - Tool call history tracking and analysis\n  - Support for complex, multi-step tool chains\n\n- **Advanced Conversation Management**:\n  - Complete conversation history tracking\n  - Filtering and viewing specific message ranges\n  - JSON export capabilities for debugging or analysis\n  - Conversation compaction for reduced token usage\n\n- **Rich User Experience**:\n  - Command completion with context-aware suggestions\n  - Colorful, formatted console output\n  - Progress indicators for long-running operations\n  - Detailed help and documentation\n\n- **Resilient Resource Management**:\n  - Proper cleanup of asyncio resources\n  - Graceful error handling\n  - Clean terminal restoration\n  - Support for multiple simultaneous server connections\n\n## üìã Prerequisites\n\n- Python 3.11 or higher\n- For OpenAI: Valid API key in `OPENAI_API_KEY` environment variable\n- For Anthropic: Valid API key in `ANTHROPIC_API_KEY` environment variable\n- For Ollama: Local Ollama installation\n- Server configuration file (default: `server_config.json`)\n- [CHUK-MCP](https://github.com/chrishayuk/chuk-mcp) protocol library\n\n## üöÄ Installation\n\n### Install from Source\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/chrishayuk/mcp-cli\ncd mcp-cli  \n```\n\n2. Install the package with development dependencies:\n\n```bash\npip install -e \".[cli,dev]\"\n```\n\n3. Run the CLI:\n\n```bash\nmcp-cli --help\n```\n\n### Using UV (Alternative Installation)\n\nIf you prefer using UV for dependency management:\n\n```bash\n# Install UV if not already installed\npip install uv\n\n# Install dependencies\nuv sync --reinstall\n\n# Run using UV\nuv run mcp-cli --help\n```\n\n## üß∞ Global Command-line Arguments\n\nGlobal options available for all modes and commands:\n\n- `--server`: Specify the server(s) to connect to (comma-separated for multiple)\n- `--config-file`: Path to server configuration file (default: `server_config.json`)\n- `--provider`: LLM provider to use (`openai`, `anthropic`, `ollama`, default: `openai`)\n- `--model`: Specific model to use (provider-dependent defaults)\n- `--disable-filesystem`: Disable filesystem access (default: true)\n\n### CLI Argument Format Issue\n\nYou might encounter a \"Missing argument 'KWARGS'\" error when running various commands. This is due to how the CLI parser is configured. To resolve this, use one of these approaches:\n\n1. Use the equals sign format for all arguments:\n   ```bash\n   mcp-cli tools call --server=sqlite\n   mcp-cli chat --server=sqlite --provider=ollama --model=llama3.2\n   ```\n\n2. Add a double-dash (`--`) after the command and before arguments:\n   ```bash\n   mcp-cli tools call -- --server sqlite\n   mcp-cli chat -- --server sqlite --provider ollama --model llama3.2\n   ```\n\n3. When using uv and multiple extra parameters, follow the 2nd step but add an empty string at the end:\n   ```\n   uv run mcp-cli chat -- --server sqlite --provider ollama --model llama3.2 \"\"\n   ```\n\nThese format issues apply to all commands (chat, interactive, tools, etc.) and are due to how the argument parser interprets positional vs. named arguments.\n\n## üåê Available Modes\n\n### 1. Chat Mode\n\nChat mode provides a natural language interface for interacting with LLMs, where the model can automatically use available tools:\n\n```bash\n# Default (makes chat the default when no other command is specified)\nuv run mcp-cli\n\n# Explicit chat mode\nuv run mcp-cli chat --server sqlite\n\n# With specific provider and model\nuv run mcp-cli chat --server sqlite --provider openai --model gpt-4o\n```\n\n### 2. Interactive Mode\n\nInteractive mode provides a command-driven shell interface for direct server operations:\n\n```bash\nuv run mcp-cli interactive --server sqlite\n```\n\n### 3. Command Mode (Cmd)\n\nCommand mode provides a Unix-friendly interface for automation and pipeline integration:\n\n```bash\nuv run mcp-cli cmd --server sqlite [options]\n```\n\n### 4. Direct Commands\n\nRun individual commands without entering an interactive mode:\n\n```bash\n# List available tools\nuv run mcp-cli tools list {} --server sqlite\n\n# Call a specific tool\nuv run mcp-cli tools call {} --server sqlite\n```\n\n## ü§ñ Using Chat Mode\n\nChat mode provides a conversational interface with the LLM, automatically using available tools when needed.\n\n### Starting Chat Mode\n\n```bash\n# Default with {} for KWARGS\nuv run mcp-cli --server sqlite\n\n# Explicit chat mode with {}\nuv run mcp-cli chat --server sqlite\n\n# With specific provider and model\nuv run mcp-cli chat --server sqlite --provider openai --model gpt-4o\n```\n\n```bash\n# Note: Be careful with the command syntax\n# Correct format without any KWARGS parameter\nuv run mcp-cli chat --server sqlite --provider ollama --model llama3.2\n\n# Or if you encounter the \"Missing argument 'KWARGS'\" error, try:\nuv run mcp-cli chat --server=sqlite --provider=ollama --model=llama3.2\n```\n\n### Chat Commands\n\nIn chat mode, use these slash commands:\n\n#### General Commands\n- `/help`: Show available commands\n- `/help <command>`: Show detailed help for a specific command\n- `/quickhelp` or `/qh`: Display a quick reference of common commands\n- `exit` or `quit`: Exit chat mode\n\n#### Provider and Model Commands\n- `/provider` or `/p`: Display or manage LLM providers\n  - `/provider`: Show current provider and model\n  - `/provider list`: List all configured providers\n  - `/provider config`: Show detailed provider configuration\n  - `/provider set <name> <key> <value>`: Set a provider configuration value\n  - `/provider <name>`: Switch to a different provider\n- `/model` or `/m`: Display or change the current model\n  - `/model`: Show current model\n  - `/model <name>`: Switch to a different model\n\n#### Tool Commands\n- `/tools`: Display all available tools with their server information\n  - `/tools --all`: Show detailed tool information including parameters\n  - `/tools --raw`: Show raw tool definitions\n- `/toolhistory` or `/th`: Show history of tool calls in the current session\n  - `/th <N>`: Show details for a specific tool call\n  - `/th -n 5`: Show only the last 5 tool calls\n  - `/th --json`: Show tool calls in JSON format\n\n#### Conversation Commands\n- `/conversation` or `/ch`: Show the conversation history\n  - `/ch <N>`: Show a specific message from history\n  - `/ch -n 5`: Show only the last 5 messages\n  - `/ch <N> --json`: Show a specific message in JSON format\n  - `/ch --json`: View the entire conversation history in raw JSON format\n- `/save <filename>`: Save conversation history to a JSON file\n- `/compact`: Condense conversation history into a summary\n\n#### Display Commands\n- `/cls`: Clear the screen while keeping conversation history\n- `/clear`: Clear both the screen and conversation history\n- `/verbose` or `/v`: Toggle between verbose and compact tool display modes\n\n#### Control Commands\n- `/interrupt`, `/stop`, or `/cancel`: Interrupt running tool execution\n- `/servers`: List connected servers and their status\n\n## üñ•Ô∏è Using Interactive Mode\n\nInteractive mode provides a command-driven shell interface for direct server interaction.\n\n### Starting Interactive Mode\n\n```bash\n# Using {} to satisfy KWARGS requirement\nmcp-cli interactive {} --server sqlite\n```\n\n### Interactive Commands\n\nIn interactive mode, use these commands:\n\n- `help`: Show available commands\n- `exit` or `quit` or `q`: Exit interactive mode\n- `clear` or `cls`: Clear the terminal screen\n- `servers` or `srv`: List connected servers with their status\n- `provider` or `p`: Manage LLM providers\n  - `provider`: Show current provider and model\n  - `provider list`: List all configured providers\n  - `provider config`: Show detailed provider configuration\n  - `provider set <name> <key> <value>`: Set a provider configuration value\n  - `provider <name>`: Switch to a different provider\n- `model` or `m`: Display or change the current model\n  - `model`: Show current model\n  - `model <name>`: Switch to a different model\n- `tools` or `t`: List available tools or call one interactively\n  - `tools --all`: Show detailed tool information\n  - `tools --raw`: Show raw JSON definitions\n  - `tools call`: Launch the interactive tool-call UI\n- `resources` or `res`: List available resources from all servers\n- `prompts` or `p`: List available prompts from all servers\n- `ping`: Ping connected servers (optionally filter by index/name)\n\n## üìÑ Using Command Mode (Cmd)\n\nCommand mode provides a Unix-friendly interface for automation and pipeline integration.\n\n### Starting Command Mode\n\n```bash\n# Using {} to satisfy KWARGS requirement\nmcp-cli cmd {} --server sqlite [options]\n```\n\n### Command Mode Options\n\n- `--input`: Input file path (use `-` for stdin)\n- `--output`: Output file path (use `-` for stdout, default)\n- `--prompt`: Prompt template (use `{{input}}` as placeholder for input)\n- `--raw`: Output raw text without formatting\n- `--tool`: Directly call a specific tool\n- `--tool-args`: JSON arguments for tool call\n- `--system-prompt`: Custom system prompt\n- `--verbose`: Enable verbose logging\n- `--provider`: Specify LLM provider\n- `--model`: Specify model to use\n\n### Command Mode Examples\n\nProcess content with LLM:\n\n```bash\n# Summarize a document (with {} for KWARGS)\nuv run mcp-cli cmd --server sqlite --input document.md --prompt \"Summarize this: {{input}}\" --output summary.md\n\n# Process stdin and output to stdout\ncat document.md | mcp-cli cmd {} --server sqlite --input - --prompt \"Extract key points: {{input}}\"\n\n# Use a specific provider and model\nuv run mcp-cli cmd {} --server sqlite --input document.md --prompt \"Summarize: {{input}}\" --provider anthropic --model claude-3-opus\n```\n\nCall tools directly:\n\n```bash\n# List database tables\nuv run mcp-cli cmd {} --server sqlite --tool list_tables --raw\n\n# Run a SQL query\nuv run mcp-cli cmd {} --server sqlite --tool read_query --tool-args '{\"query\": \"SELECT COUNT(*) FROM users\"}'\n```\n\nBatch processing:\n\n```bash\n# Process multiple files with GNU Parallel\nls *.md | parallel mcp-cli cmd --server sqlite --input {} --output {}.summary.md --prompt \"Summarize: {{input}}\"\n```\n\n## üîß Direct CLI Commands\n\nRun individual commands without entering interactive mode:\n\n### Provider Commands\n\n```bash\n# Show current provider configuration\nmcp-cli provider show\n\n# List all configured providers\nmcp-cli provider list\n\n# Show detailed provider configuration\nmcp-cli provider config\n\n# Set a configuration value\nmcp-cli provider set <provider_name> <key> <value>\n# Example: mcp-cli provider set openai api_key \"sk-...\"\n```\n\n### Tools Commands\n\n```bash\n# List all tools (using {} to satisfy KWARGS requirement)\nuv run mcp-cli tools list {} --server sqlite\n\n# Show detailed tool information\nuv run mcp-cli tools list {} --server sqlite --all\n\n# Show raw tool definitions\nuv run mcp-cli tools list {} --server sqlite --raw\n\n# Call a specific tool interactively\nuv run mcp-cli tools call {} --server sqlite\n```\n\n### Resources and Prompts Commands\n\n```bash\n# List available resources\nuv run mcp-cli resources list {} --server sqlite\n\n# List available prompts\nuv run mcp-cli prompts list {} --server sqlite\n```\n\n### Server Commands\n\n```bash\n# Ping all servers\nuv run mcp-cli ping {} --server sqlite\n\n# Ping specific server(s)\nuv run mcp-cli ping {} --server sqlite,another-server\n```\n\n## üìÇ Server Configuration\n\nCreate a `server_config.json` file with your server configurations:\n\n```json\n{\n  \"mcpServers\": {\n    \"sqlite\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"mcp_server.sqlite_server\"],\n      \"env\": {\n        \"DATABASE_PATH\": \"your_database.db\"\n      }\n    },\n    \"another-server\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"another_server_module\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\n## üîê Provider Configuration\n\nProvider configurations are stored with these key settings:\n\n- `api_key`: API key for authentication\n- `api_base`: Base URL for API requests\n- `default_model`: Default model to use with this provider\n- Other provider-specific settings\n\n### Environment Variables\n\nYou can also set the default provider and model using environment variables:\n\n```bash\nexport LLM_PROVIDER=openai\nexport LLM_MODEL=gpt-4o-mini\n```\n\n### Configuration Example\n\nThe provider configuration is typically stored in a JSON file and looks like:\n\n```json\n{\n  \"openai\": {\n    \"api_key\": \"sk-...\",\n    \"api_base\": \"https://api.openai.com/v1\",\n    \"default_model\": \"gpt-4o-mini\"\n  },\n  \"anthropic\": {\n    \"api_key\": \"sk-...\",\n    \"api_base\": \"https://api.anthropic.com\",\n    \"default_model\": \"claude-3-opus\"\n  },\n  \"ollama\": {\n    \"api_base\": \"http://localhost:11434\",\n    \"default_model\": \"llama3.2\"\n  }\n}\n```\n\n## üìà Advanced Usage Examples\n\n### Provider and Model Selection\n\nYou can change providers or models during a session:\n\n```\n# In chat mode\n> /provider\nCurrent provider: openai\nCurrent model: gpt-4o-mini\nTo change provider: /provider <provider_name>\n\n> /provider list\nAvailable Providers\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Provider  ‚îÉ Default Model  ‚îÉ API Base                        ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ openai    ‚îÇ gpt-4o-mini    ‚îÇ https://api.openai.com/v1       ‚îÇ\n‚îÇ anthropic ‚îÇ claude-3-opus  ‚îÇ https://api.anthropic.com       ‚îÇ\n‚îÇ ollama    ‚îÇ llama3.2       ‚îÇ http://localhost:11434          ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n> /provider anthropic\nSwitched to provider: anthropic with model: claude-3-opus\nLLM client updated successfully\n\n> /model claude-3-sonnet\nSwitched to model: claude-3-sonnet\n```\n\n### Working with Tools in Chat Mode\n\nIn chat mode, simply ask questions that require tool usage, and the LLM will automatically call the appropriate tools:\n\n```\nYou: What tables are available in the database?\n[Tool Call: list_tables]\nAssistant: There's one table in the database named products. How would you like to proceed?\n\nYou: Select top 10 products ordered by price in descending order\n[Tool Call: read_query]\nAssistant: Here are the top 10 products ordered by price in descending order:\n  1 Mini Drone - $299.99\n  2 Smart Watch - $199.99\n  3 Portable SSD - $179.99\n  ...\n```\n\n### Using Conversation Management\n\nThe MCP CLI provides powerful conversation history management:\n\n```\n> /conversation\nConversation History (12 messages)\n# | Role      | Content\n1 | system    | You are an intelligent assistant capable of using t...\n2 | user      | What tables are available in the database?\n3 | assistant | Let me check for you.\n...\n\n> /save conversation.json\nConversation saved to conversation.json\n\n> /compact\nConversation history compacted with summary.\n```\n\n## üõ†Ô∏è Implementation Details\n\nThe provider configuration is managed by the `ProviderConfig` class, which:\n- Loads/saves configuration from a local file\n- Manages active provider and model settings\n- Provides helper methods for retrieving configuration values\n\nThe LLM client is created using the `get_llm_client` function, which instantiates the appropriate client based on the provider and model settings.\n\n## üì¶ Dependencies\n\nThe CLI is organized with optional dependency groups:\n\n- **cli**: Rich terminal UI, command completion, and provider integrations\n- **dev**: Development tools and testing utilities\n- **wasm**: (Reserved for future WebAssembly support)\n- **chuk-mcp**: Protocol implementation library (core dependency)\n\nInstall with specific extras using:\n```bash\npip install \"mcp-cli[cli]\"     # Basic CLI features\npip install \"mcp-cli[cli,dev]\" # CLI with development tools\n```\n\n## ü§ù Contributing\n\nContributions are welcome! Please follow these steps:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## üìú License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## üôè Acknowledgments\n\n- [Rich](https://github.com/Textualize/rich) for beautiful terminal formatting\n- [Typer](https://typer.tiangolo.com/) for CLI argument parsing\n- [Prompt Toolkit](https://github.com/prompt-toolkit/python-prompt-toolkit) for interactive input\n- [CHUK-MCP](https://github.com/chrishayuk/chuk-mcp) for the core protocol implementation\n"
}