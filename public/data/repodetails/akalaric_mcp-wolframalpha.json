{
  "mcp_name": "ricocf/mcp-wolframalpha",
  "mcp_description": "üêç üè† ‚òÅÔ∏è - An MCP server lets AI assistants use the Wolfram Alpha API for real-time access to computational knowledge and data.",
  "mcp_id": "akalaric_mcp-wolframalpha",
  "fetch_timestamp": "2025-06-23T07:58:13.970501Z",
  "github_url": "https://github.com/ricocf/mcp-wolframalpha",
  "repository": {
    "name": "mcp-wolframalpha",
    "full_name": "akalaric/mcp-wolframalpha",
    "description": "A Python-powered Model Context Protocol MCP server and client that uses Wolfram Alpha via API.",
    "html_url": "https://github.com/akalaric/mcp-wolframalpha",
    "created_at": "2025-04-14T11:17:45Z",
    "updated_at": "2025-06-22T20:48:30Z",
    "pushed_at": "2025-06-18T21:31:27Z",
    "size": 603,
    "stargazers_count": 24,
    "watchers_count": 24,
    "forks_count": 3,
    "open_issues_count": 1,
    "language": "Python",
    "license": "MIT License",
    "topics": [],
    "default_branch": "main",
    "owner": {
      "login": "akalaric",
      "type": "User",
      "avatar_url": "https://avatars.githubusercontent.com/u/67761683?v=4",
      "html_url": "https://github.com/akalaric"
    },
    "has_issues": true,
    "has_projects": true,
    "has_downloads": true,
    "has_wiki": true,
    "has_pages": false,
    "archived": false,
    "disabled": false,
    "visibility": "public",
    "network_count": 3,
    "subscribers_count": 1,
    "languages": {
      "Python": 15736,
      "Dockerfile": 451
    },
    "language_percentages": {
      "Python": 97.21,
      "Dockerfile": 2.79
    },
    "pull_requests_count": 4,
    "contributors_count": 1
  },
  "readme": "# MCP Wolfram Alpha (Server + Client)\nSeamlessly integrate Wolfram Alpha into your chat applications.\n\nThis project implements an MCP (Model Context Protocol) server designed to interface with the Wolfram Alpha API. It enables chat-based applications to perform computational queries and retrieve structured knowledge, facilitating advanced conversational capabilities.\n\nIncluded is an MCP-Client example utilizing Gemini via LangChain, demonstrating how to connect large language models to the MCP server for real-time interactions with Wolfram Alpha‚Äôs knowledge engine.\n\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/akalaric/mcp-wolframalpha)\n---\n\n## Features\n\n-  **Wolfram|Alpha Integration** for math, science, and data queries.\n\n-  **Modular Architecture** Easily extendable to support additional APIs and functionalities.\n\n-  **Multi-Client Support** Seamlessly handle interactions from multiple clients or interfaces.\n\n-  **MCP-Client example** using Gemini (via LangChain).\n-  **UI Support** using Gradio for a user-friendly web interface to interact with Google AI and Wolfram Alpha MCP server.\n\n---\n\n##  Installation\n\n\n### Clone the Repo\n   ```bash\n   git clone https://github.com/ricocf/mcp-wolframalpha.git\n\n   cd mcp-wolframalpha\n   ```\n  \n\n### Set Up Environment Variables\n\nCreate a .env file based on the example:\n\n- WOLFRAM_API_KEY=your_wolframalpha_appid\n\n- GeminiAPI=your_google_gemini_api_key *(Optional if using Client method below.)*\n\n### Install Requirements\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n### Configuration\n\nTo use with the VSCode MCP Server:\n1.  Create a configuration file at `.vscode/mcp.json` in your project root.\n2.  Use the example provided in `configs/vscode_mcp.json` as a template.\n3.  For more details, refer to the [VSCode MCP Server Guide](https://sebastian-petrus.medium.com/vscode-mcp-server-42286eed3ee7).\n\nTo use with Claude Desktop:\n```json\n{\n  \"mcpServers\": {\n    \"WolframAlphaServer\": {\n      \"command\": \"python3\",\n      \"args\": [\n        \"/path/to/src/core/server.py\"\n      ]\n    }\n  }\n}\n```\n## Client Usage Example\n\nThis project includes an LLM client that communicates with the MCP server.\n\n#### Run with Gradio UI\n- Required: GeminiAPI\n- Provides a local web interface to interact with Google AI and Wolfram Alpha.\n- To run the client directly from the command line:\n```bash\npython main.py --ui\n```\n#### Docker\nTo build and run the client inside a Docker container:\n```bash\ndocker build -t wolframalphaui -f .devops/ui.Dockerfile .\n\ndocker run wolframalphaui\n```\n#### UI\n- Intuitive interface built with Gradio to interact with both Google AI (Gemini) and the Wolfram Alpha MCP server.\n- Allows users to switch between Wolfram Alpha, Google AI (Gemini), and query history.\n  \n![UI](configs/gradio_ui.png)\n\n#### Run as CLI Tool\n- Required: GeminiAPI\n- To run the client directly from the command line:\n```bash\npython main.py\n```\n#### Docker\nTo build and run the client inside a Docker container:\n```bash\ndocker build -t wolframalpha -f .devops/llm.Dockerfile .\n\ndocker run -it wolframalpha\n```\n\n   \n\n"
}