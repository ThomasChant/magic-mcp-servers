{
  "mcp_name": "RIKTESH89/mcp_cli_filesystem",
  "mcp_description": "A command-line interface for interacting with Model Context Provider servers, enabling seamless communication with LLMs through the CHUK-MCP protocol library.",
  "mcp_id": "RIKTESH89_mcp_cli_filesystem",
  "fetch_timestamp": "2025-06-23T07:59:20.670558Z",
  "github_url": "https://github.com/RIKTESH89/mcp_cli_filesystem",
  "repository": {
    "name": "mcp_cli_filesystem",
    "full_name": "RIKTESH89/mcp_cli_filesystem",
    "description": null,
    "html_url": "https://github.com/RIKTESH89/mcp_cli_filesystem",
    "created_at": "2025-04-01T19:38:10Z",
    "updated_at": "2025-04-01T19:43:23Z",
    "pushed_at": "2025-04-01T19:43:19Z",
    "size": 87,
    "stargazers_count": 0,
    "watchers_count": 0,
    "forks_count": 0,
    "open_issues_count": 0,
    "language": "Python",
    "license": "Other",
    "topics": [],
    "default_branch": "main",
    "owner": {
      "login": "RIKTESH89",
      "type": "User",
      "avatar_url": "https://avatars.githubusercontent.com/u/170694793?v=4",
      "html_url": "https://github.com/RIKTESH89"
    },
    "has_issues": true,
    "has_projects": true,
    "has_downloads": true,
    "has_wiki": true,
    "has_pages": false,
    "archived": false,
    "disabled": false,
    "visibility": "public",
    "network_count": 0,
    "subscribers_count": 1,
    "languages": {
      "Python": 169012
    },
    "language_percentages": {
      "Python": 100
    },
    "pull_requests_count": 0,
    "contributors_count": 1
  },
  "readme": "# MCP CLI - Model Context Provider Command Line Interface\nA powerful, feature-rich command-line interface for interacting with Model Context Provider servers. This client enables seamless communication with LLMs through integration with the [CHUK-MCP protocol library](https://github.com/chrishayuk/chuk-mcp) which is a pyodide compatible pure python protocol implementation of MCP, supporting tool usage, conversation management, and multiple operational modes.\n\n## üîÑ Protocol Implementation\n\nThe core protocol implementation has been moved to a separate package at:\n**[https://github.com/chrishayuk/chuk-mcp](https://github.com/chrishayuk/chuk-mcp)**\n\nThis CLI is built on top of the protocol library, focusing on providing a rich user experience while the protocol library handles the communication layer.\n\n## üåü Features\n\n- **Multiple Operational Modes**:\n  - **Chat Mode**: Conversational interface with direct LLM interaction and automated tool usage\n  - **Interactive Mode**: Command-driven interface for direct server operations\n  - **Command Mode**: Unix-friendly mode for scriptable automation and pipelines\n  - **Direct Commands**: Run individual commands without entering interactive mode\n\n- **Multi-Provider Support**:\n  - OpenAI integration (`gpt-4o-mini`, `gpt-4o`, `gpt-4-turbo`, etc.)\n  - Ollama integration (`llama3.2`, `qwen2.5-coder`, etc.)\n  - Extensible architecture for additional providers\n\n- **Robust Tool System**:\n  - Automatic discovery of server-provided tools\n  - Server-aware tool execution\n  - Tool call history tracking and analysis\n  - Support for complex, multi-step tool chains\n\n- **Advanced Conversation Management**:\n  - Complete conversation history tracking\n  - Filtering and viewing specific message ranges\n  - JSON export capabilities for debugging or analysis\n  - Conversation compaction for reduced token usage\n\n- **Rich User Experience**:\n  - Command completion with context-aware suggestions\n  - Colorful, formatted console output\n  - Progress indicators for long-running operations\n  - Detailed help and documentation\n\n- **Resilient Resource Management**:\n  - Proper cleanup of asyncio resources\n  - Graceful error handling\n  - Clean terminal restoration\n  - Support for multiple simultaneous server connections\n\n## üìã Prerequisites\n\n- Python 3.11 or higher\n- For OpenAI: Valid API key in `OPENAI_API_KEY` environment variable\n- For Ollama: Local Ollama installation\n- Server configuration file (default: `server_config.json`)\n- [CHUK-MCP](https://github.com/chrishayuk/chuk-mcp) protocol library\n\n## üöÄ Installation\n\n### Install from Source\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/chrishayuk/mcp-cli\ncd mcp-cli\n```\n\n2. Install the package with development dependencies:\n\n```bash\npip install -e \".[cli,dev]\"\n```\n\n3. Run the CLI:\n\n```bash\nmcp-cli --help\n```\n\n### Using UV (Alternative Installation)\n\nIf you prefer using UV for dependency management:\n\n```bash\n# Install UV if not already installed\npip install uv\n\n# Install dependencies\nuv sync --reinstall\n\n# Run using UV\nuv run mcp-cli --help\n```\n\n## üß∞ Command-line Arguments\n\nGlobal options available for all commands:\n\n- `--server`: Specify the server(s) to connect to (comma-separated for multiple)\n- `--config-file`: Path to server configuration file (default: `server_config.json`)\n- `--provider`: LLM provider to use (`openai` or `ollama`, default: `openai`)\n- `--model`: Specific model to use (provider-dependent defaults)\n- `--disable-filesystem`: Disable filesystem access (default: true)\n\n## ü§ñ Using Chat Mode\n\nChat mode provides a conversational interface with the LLM, automatically using available tools when needed:\n\n```bash\nmcp-cli chat --server sqlite\n```\n\nWith specific provider and model:\n\n```bash\nmcp-cli chat --server sqlite --provider openai --model gpt-4o\n```\n\n```bash\nmcp-cli chat --server sqlite --provider ollama --model llama3.2\n```\n\n### Chat Commands\n\nIn chat mode, use these slash commands:\n\n#### General Commands\n- `/help`: Show available commands\n- `/help <command>`: Show detailed help for a specific command\n- `/quickhelp` or `/qh`: Display a quick reference of common commands\n- `exit` or `quit`: Exit chat mode\n\n#### Tool Commands\n- `/tools`: Display all available tools with their server information\n  - `/tools --all`: Show detailed tool information including parameters\n  - `/tools --raw`: Show raw tool definitions\n- `/toolhistory` or `/th`: Show history of tool calls in the current session\n  - `/th <N>`: Show details for a specific tool call\n  - `/th -n 5`: Show only the last 5 tool calls\n  - `/th --json`: Show tool calls in JSON format\n\n#### Conversation Commands\n- `/conversation` or `/ch`: Show the conversation history\n  - `/ch <N>`: Show a specific message from history\n  - `/ch -n 5`: Show only the last 5 messages\n  - `/ch <N> --json`: Show a specific message in JSON format\n  - `/ch --json`: View the entire conversation history in raw JSON format\n- `/save <filename>`: Save conversation history to a JSON file\n- `/compact`: Condense conversation history into a summary\n\n#### Display Commands\n- `/cls`: Clear the screen while keeping conversation history\n- `/clear`: Clear both the screen and conversation history\n- `/verbose` or `/v`: Toggle between verbose and compact tool display modes\n\n#### Control Commands\n- `/interrupt`, `/stop`, or `/cancel`: Interrupt running tool execution\n- `/provider <n>`: Change the current LLM provider \n- `/model <n>`: Change the current LLM model\n- `/servers`: List connected servers and their status\n\n## üñ•Ô∏è Using Interactive Mode\n\nInteractive mode provides a command-line interface with slash commands for direct server interaction:\n\n```bash\nmcp-cli interactive --server sqlite\n```\n\n### Interactive Commands\n\nIn interactive mode, use these commands:\n\n- `/ping`: Check if server is responsive\n- `/prompts`: List available prompts\n- `/tools`: List available tools\n- `/tools-all`: Show detailed tool information with parameters\n- `/tools-raw`: Show raw tool definitions in JSON\n- `/resources`: List available resources\n- `/chat`: Enter chat mode\n- `/cls`: Clear the screen\n- `/clear`: Clear the screen and show welcome message\n- `/help`: Show help message\n- `/exit` or `/quit`: Exit the program\n\n## üìÑ Using Command Mode\n\nCommand mode provides a Unix-friendly interface for automation and pipeline integration:\n\n```bash\nmcp-cli cmd --server sqlite [options]\n```\n\nThis mode is designed for scripting, batch processing, and direct integration with other Unix tools.\n\n### Command Mode Options\n\n- `--input`: Input file path (use `-` for stdin)\n- `--output`: Output file path (use `-` for stdout, default)\n- `--prompt`: Prompt template (use `{{input}}` as placeholder for input)\n- `--raw`: Output raw text without formatting\n- `--tool`: Directly call a specific tool\n- `--tool-args`: JSON arguments for tool call\n- `--system-prompt`: Custom system prompt\n\n### Command Mode Examples\n\nProcess content with LLM:\n\n```bash\n# Summarize a document\nmcp-cli cmd --server sqlite --input document.md --prompt \"Summarize this: {{input}}\" --output summary.md\n\n# Process stdin and output to stdout\ncat document.md | mcp-cli cmd --server sqlite --input - --prompt \"Extract key points: {{input}}\"\n```\n\nCall tools directly:\n\n```bash\n# List database tables\nmcp-cli cmd --server sqlite --tool list_tables --raw\n\n# Run a SQL query\nmcp-cli cmd --server sqlite --tool read_query --tool-args '{\"query\": \"SELECT COUNT(*) FROM users\"}'\n```\n\nBatch processing:\n\n```bash\n# Process multiple files with GNU Parallel\nls *.md | parallel mcp-cli cmd --server sqlite --input {} --output {}.summary.md --prompt \"Summarize: {{input}}\"\n```\n\n## üîß Direct Commands\n\nRun individual commands without entering interactive mode:\n\n```bash\n# List available tools\nmcp-cli tools list --server sqlite\n\n# Call a specific tool\nmcp-cli tools call --server sqlite\n\n# List available prompts\nmcp-cli prompts list --server sqlite\n\n# Check server connectivity\nmcp-cli ping --server sqlite\n\n# List available resources\nmcp-cli resources list --server sqlite\n```\n\n## üìÇ Server Configuration\n\nCreate a `server_config.json` file with your server configurations:\n\n```json\n{\n  \"mcpServers\": {\n    \"sqlite\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"mcp_server.sqlite_server\"],\n      \"env\": {\n        \"DATABASE_PATH\": \"your_database.db\"\n      }\n    },\n    \"another-server\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"another_server_module\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\n## üèóÔ∏è Project Structure\n\n```\nsrc/\n‚îú‚îÄ‚îÄ mcp_cli/\n‚îÇ   ‚îú‚îÄ‚îÄ chat/                  # Chat mode implementation\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commands/          # Chat slash commands\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py    # Command registration system\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversation.py  # Conversation management\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversation_history.py   \n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ exit.py              \n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ help.py              \n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ help_text.py         \n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py            \n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ servers.py           \n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tool_history.py      \n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tools.py             \n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat_context.py    # Chat session state management\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat_handler.py    # Main chat loop handler\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ command_completer.py  # Command completion\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversation.py    # Conversation processor\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ system_prompt.py   # System prompt generator\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tool_processor.py  # Tool handling\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ui_manager.py      # User interface\n‚îÇ   ‚îú‚îÄ‚îÄ commands/              # CLI commands\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py            # Chat command\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cmd.py             # Command mode\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ interactive.py     # Interactive mode\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ping.py            # Ping command\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts.py         # Prompts commands\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ register_commands.py  # Command registration\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ resources.py       # Resources commands\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tools.py           # Tools commands\n‚îÇ   ‚îú‚îÄ‚îÄ llm/                   # LLM client implementations\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ providers/         # Provider-specific clients\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py        # Base LLM client\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ openai_client.py  # OpenAI implementation\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_client.py      # Client factory\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ system_prompt_generator.py  # Prompt generator\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tools_handler.py   # Tools handling\n‚îÇ   ‚îú‚îÄ‚îÄ ui/                    # User interface components\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ colors.py          # Color definitions\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ui_helpers.py      # UI utilities\n‚îÇ   ‚îú‚îÄ‚îÄ cli_options.py         # CLI options processing\n‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Configuration loader\n‚îÇ   ‚îú‚îÄ‚îÄ main.py                # Main entry point\n‚îÇ   ‚îî‚îÄ‚îÄ run_command.py         # Command execution\n```\n\n## üìà Advanced Usage\n\n### Tool Execution\n\nThe MCP CLI can automatically execute tools provided by the server. In chat mode, simply request information that requires tool usage, and the LLM will automatically select and call the appropriate tools.\n\nExample conversation:\n\n```\nYou: What tables are available in the database?\nAssistant: Let me check for you.\n[Tool Call: list_tables]\nI found the following tables in the database:\n- users\n- products\n- orders\n- categories\n\nYou: How many users do we have?\nAssistant: I'll query the database for that information.\n[Tool Call: read_query]\nThere are 873 users in the database.\n```\n\n### Scripting with Command Mode\n\nCommand mode enables powerful automation through shell scripts:\n\n```bash\n#!/bin/bash\n# Example script to analyze multiple documents\n\n# Process all markdown files in the current directory\nfor file in *.md; do\n  echo \"Processing $file...\"\n  \n  # Generate summary\n  mcp-cli cmd --server sqlite --input \"$file\" \\\n    --prompt \"Summarize this document: {{input}}\" \\\n    --output \"${file%.md}.summary.md\"\n  \n  # Extract entities\n  mcp-cli cmd --server sqlite --input \"$file\" \\\n    --prompt \"Extract all company names, people, and locations from this text: {{input}}\" \\\n    --output \"${file%.md}.entities.txt\" --raw\ndone\n\n# Create a combined report\necho \"Creating final report...\"\ncat *.entities.txt | mcp-cli cmd --server sqlite --input - \\\n  --prompt \"Analyze these entities and identify the most frequently mentioned:\" \\\n  --output report.md\n```\n\n### Conversation Management\n\nTrack and manage your conversation history:\n\n```\n> /conversation\nConversation History (12 messages)\n# | Role      | Content\n1 | system    | You are an intelligent assistant capable of using t...\n2 | user      | What tables are available in the database?\n3 | assistant | Let me check for you.\n4 | assistant | [Tool call: list_tables]\n...\n\n> /conversation 4\nMessage #4 (Role: assistant)\n[Tool call: list_tables]\nTool Calls:\n  1. ID: call_list_tables_12345678, Type: function, Name: list_tables\n     Arguments: {}\n\n> /save conversation.json\nConversation saved to conversation.json\n\n> /compact\nConversation history compacted with summary.\nSummary:\nThe user asked about database tables, and I listed the available tables (users, products, orders, categories). The user then asked about the number of users, and I queried the database to find there are 873 users.\n```\n\n## üì¶ Dependencies\n\nThe CLI is organized with optional dependency groups:\n\n- **cli**: Rich terminal UI, command completion, and provider integrations\n- **dev**: Development tools and testing utilities\n- **wasm**: (Reserved for future WebAssembly support)\n- **chuk-mcp**: Protocol implementation library (core dependency)\n\nInstall with specific extras using:\n```bash\npip install \"mcp-cli[cli]\"     # Basic CLI features\npip install \"mcp-cli[cli,dev]\" # CLI with development tools\n```\n\n## ü§ù Contributing\n\nContributions are welcome! Please follow these steps:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n## üìú License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## üôè Acknowledgments\n\n- [Anthropic Claude](https://www.anthropic.com/claude) for assistance with code development\n- [Rich](https://github.com/Textualize/rich) for beautiful terminal formatting\n- [Typer](https://typer.tiangolo.com/) for CLI argument parsing\n- [Prompt Toolkit](https://github.com/prompt-toolkit/python-prompt-toolkit) for interactive input\n- [CHUK-MCP](https://github.com/chrishayuk/chuk-mcp) for the core protocol implementation"
}